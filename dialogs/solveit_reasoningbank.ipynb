{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8be718e9",
   "metadata": {},
   "source": [
    "# Experiments with Reasoningbank as a Solveit Tool\n",
    "\n",
    "- Date 12-06-2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3583d4ce",
   "metadata": {
    "time_run": "2025-12-06T16:05:45.748394+00:00"
   },
   "outputs": [],
   "source": [
    "from dialoghelper import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7bdf2b",
   "metadata": {},
   "source": [
    "Tools available from `dialoghelper`:\n",
    "\n",
    "- &`curr_dialog`: Get the current dialog info.\n",
    "- &`msg_idx`: Get absolute index of message in dialog.\n",
    "- &`add_html`: Send HTML to the browser to be swapped into the DOM using hx-swap-oob.\n",
    "- &`find_msg_id`: Get the current message id.\n",
    "- &`find_msgs`: Find messages in current specific dialog that contain the given information.\n",
    "  - (solveit can often get this id directly from its context, and will not need to use this if the required information is already available to it.)\n",
    "- &`read_msg`: Get the message indexed in the current dialog.\n",
    "  - To get the exact message use `n=0` and `relative=True` together with `msgid`.\n",
    "  - To get a relative message use `n` (relative position index).\n",
    "  - To get the nth message use `n` with `relative=False`, e.g `n=0` first message, `n=-1` last message.\n",
    "- &`del_msg`: Delete a message from the dialog.\n",
    "- &`add_msg`: Add/update a message to the queue to show after code execution completes.\n",
    "- &`update_msg`: Update an existing message.\n",
    "- &`url2note`: Read URL as markdown, and add a note below current message with the result\n",
    "- &`msg_insert_line`: Insert text at a specific location in a message.\n",
    "- &`msg_str_replace`: Find and replace text in a message.\n",
    "- &`msg_strs_replace`: Find and replace multiple strings in a message.\n",
    "- &`msg_replace_lines`: Replace a range of lines in a message with new content.\n",
    "  - Always first use `read_msg( msgid=msgid, n=0, relative=True, nums=True)` to view the content with line numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34edb7ca",
   "metadata": {},
   "source": [
    "Tools available from `fastcore.tools`:\n",
    "\n",
    "- &`rg`: Run the `rg` command with the args in `argstr` (no need to backslash escape)\n",
    "- &`sed`: Run the `sed` command with the args in `argstr` (e.g for reading a section of a file)\n",
    "- &`view`: View directory or file contents with optional line range and numbers\n",
    "- &`create`: Creates a new file with the given content at the specified path\n",
    "- &`insert`: Insert new_str at specified line number\n",
    "- &`str_replace`: Replace first occurrence of old_str with new_str in file\n",
    "- &`strs_replace`: Replace for each str pair in old_strs,new_strs\n",
    "- &`replace_lines`: Replace lines in file using start and end line-numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52a7749",
   "metadata": {
    "time_run": "2025-12-06T16:06:30.817894+00:00"
   },
   "outputs": [],
   "source": [
    "from fastcore.tools import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445dd06e",
   "metadata": {
    "time_run": "2025-12-06T16:17:06.121517+00:00"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_7194021f',\n",
       " '_a7c0438a',\n",
       " '_44ccc749',\n",
       " '_3be6958f',\n",
       " '_ecc9a104',\n",
       " '_339d39c2',\n",
       " '_8725e88e',\n",
       " '_552f5c4f',\n",
       " '_137923aa',\n",
       " '_28378840',\n",
       " '_69cf253e',\n",
       " '_a9d2aaad',\n",
       " '_e602cb5a',\n",
       " '_fc13fcda',\n",
       " '_609c00b6',\n",
       " '_8471a5d7',\n",
       " '_ed8d2e05',\n",
       " '_4c7d975d',\n",
       " '_6f7b196f',\n",
       " '_0c5837d4',\n",
       " '_1beec53c',\n",
       " '_61613034',\n",
       " '_665bd30b',\n",
       " '_64e5c0db',\n",
       " '_529d6e43',\n",
       " '_c68d3510',\n",
       " '_37f191f4',\n",
       " '_2ecc52ab',\n",
       " '_af2b3f9d',\n",
       " '_6b53e875',\n",
       " '_a3943613',\n",
       " '_d1492496',\n",
       " '_f6bbf0dd',\n",
       " '_8be92f79',\n",
       " '_005be9be',\n",
       " '_f36f2779',\n",
       " '_93babc3c',\n",
       " '_f91b9846',\n",
       " '_8768e23c',\n",
       " '_a20e9ebb',\n",
       " '_0c6679ea',\n",
       " '_0e003596',\n",
       " '_85e46d98',\n",
       " '_01834630']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url2note(\"https://arxiv.org/html/2509.25140v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7194021f",
   "metadata": {},
   "source": [
    "  1. [1 Introduction](https://arxiv.org/html/2509.25140v1#S1 \"In ReasoningBank: Scaling Agent Self-Evolving with Reasoning Memory\")\n",
    "  2. [2 Related Work](https://arxiv.org/html/2509.25140v1#S2 \"In ReasoningBank: Scaling Agent Self-Evolving with Reasoning Memory\")\n",
    "  3. [3 Methodology](https://arxiv.org/html/2509.25140v1#S3 \"In ReasoningBank: Scaling Agent Self-Evolving with Reasoning Memory\")\n",
    "     1. [3.1 Problem Formulation](https://arxiv.org/html/2509.25140v1#S3.SS1 \"In 3 Methodology ‣ ReasoningBank: Scaling Agent Self-Evolving with Reasoning Memory\")\n",
    "     2. [3.2 ReasoningBank](https://arxiv.org/html/2509.25140v1#S3.SS2 \"In 3 Methodology ‣ ReasoningBank: Scaling Agent Self-Evolving with Reasoning Memory\")\n",
    "     3. [3.3 MaTTS: Memory-aware Test-Time Scaling](https://arxiv.org/html/2509.25140v1#S3.SS3 \"In 3 Methodology ‣ ReasoningBank: Scaling Agent Self-Evolving with Reasoning Memory\")\n",
    "  4. [4 Experiments](https://arxiv.org/html/2509.25140v1#S4 \"In ReasoningBank: Scaling Agent Self-Evolving with Reasoning Memory\")\n",
    "     1. [4.1 Setup](https://arxiv.org/html/2509.25140v1#S4.SS1 \"In 4 Experiments ‣ ReasoningBank: Scaling Agent Self-Evolving with Reasoning Memory\")\n",
    "     2. [4.2 Results of ReasoningBank](https://arxiv.org/html/2509.25140v1#S4.SS2 \"In 4 Experiments ‣ ReasoningBank: Scaling Agent Self-Evolving with Reasoning Memory\")\n",
    "     3. [4.3 Results of MaTTS](https://arxiv.org/html/2509.25140v1#S4.SS3 \"In 4 Experiments ‣ ReasoningBank: Scaling Agent Self-Evolving with Reasoning Memory\")\n",
    "     4. [4.4 Synergy of Memory and Test-Time Scaling](https://arxiv.org/html/2509.25140v1#S4.SS4 \"In 4 Experiments ‣ ReasoningBank: Scaling Agent Self-Evolving with Reasoning Memory\")\n",
    "  5. [5 Analysis](https://arxiv.org/html/2509.25140v1#S5 \"In ReasoningBank: Scaling Agent Self-Evolving with Reasoning Memory\")\n",
    "     1. [5.1 Emergent behaviors with ReasoningBank](https://arxiv.org/html/2509.25140v1#S5.SS1 \"In 5 Analysis ‣ ReasoningBank: Scaling Agent Self-Evolving with Reasoning Memory\")\n",
    "     2. [5.2 Incorporating failure trajectories](https://arxiv.org/html/2509.25140v1#S5.SS2 \"In 5 Analysis ‣ ReasoningBank: Scaling Agent Self-Evolving with Reasoning Memory\")\n",
    "     3. [5.3 Efficiency Study](https://arxiv.org/html/2509.25140v1#S5.SS3 \"In 5 Analysis ‣ ReasoningBank: Scaling Agent Self-Evolving with Reasoning Memory\")\n",
    "  6. [6 Conclusion](https://arxiv.org/html/2509.25140v1#S6 \"In ReasoningBank: Scaling Agent Self-Evolving with Reasoning Memory\")\n",
    "  7. [7 Acknowledgments](https://arxiv.org/html/2509.25140v1#S7 \"In ReasoningBank: Scaling Agent Self-Evolving with Reasoning Memory\")\n",
    "  8. [A Experiment Details](https://arxiv.org/html/2509.25140v1#A1 \"In ReasoningBank: Scaling Agent Self-Evolving with Reasoning Memory\")\n",
    "     1. [A.1 Prompts Used for ReasoningBank](https://arxiv.org/html/2509.25140v1#A1.SS1 \"In Appendix A Experiment Details ‣ ReasoningBank: Scaling Agent Self-Evolving with Reasoning Memory\")\n",
    "        1. [Memory Extraction.](https://arxiv.org/html/2509.25140v1#A1.SS1.SSS0.Px1 \"In A.1 Prompts Used for ReasoningBank ‣ Appendix A Experiment Details ‣ ReasoningBank: Scaling Agent Self-Evolving with Reasoning Memory\")\n",
    "        2. [LLM-as-a-Judge for Correctness Signals.](https://arxiv.org/html/2509.25140v1#A1.SS1.SSS0.Px2 \"In A.1 Prompts Used for ReasoningBank ‣ Appendix A Experiment Details ‣ ReasoningBank: Scaling Agent Self-Evolving with Reasoning Memory\")\n",
    "     2. [A.2 Implementation Details](https://arxiv.org/html/2509.25140v1#A1.SS2 \"In Appendix A Experiment Details ‣ ReasoningBank: Scaling Agent Self-Evolving with Reasoning Memory\")\n",
    "        1. [Memory Extraction.](https://arxiv.org/html/2509.25140v1#A1.SS2.SSS0.Px1 \"In A.2 Implementation Details ‣ Appendix A Experiment Details ‣ ReasoningBank: Scaling Agent Self-Evolving with Reasoning Memory\")\n",
    "        2. [Memory Retrieval and Response Generation.](https://arxiv.org/html/2509.25140v1#A1.SS2.SSS0.Px2 \"In A.2 Implementation Details ‣ Appendix A Experiment Details ‣ ReasoningBank: Scaling Agent Self-Evolving with Reasoning Memory\")\n",
    "        3. [Memory Consolidation.](https://arxiv.org/html/2509.25140v1#A1.SS2.SSS0.Px3 \"In A.2 Implementation Details ‣ Appendix A Experiment Details ‣ ReasoningBank: Scaling Agent Self-Evolving with Reasoning Memory\")\n",
    "        4. [ReasoningBank Storage](https://arxiv.org/html/2509.25140v1#A1.SS2.SSS0.Px4 \"In A.2 Implementation Details ‣ Appendix A Experiment Details ‣ ReasoningBank: Scaling Agent Self-Evolving with Reasoning Memory\")\n",
    "     3. [A.3 MaTTS Details](https://arxiv.org/html/2509.25140v1#A1.SS3 \"In Appendix A Experiment Details ‣ ReasoningBank: Scaling Agent Self-Evolving with Reasoning Memory\")\n",
    "        1. [Prompt Used for MaTTS](https://arxiv.org/html/2509.25140v1#A1.SS3.SSS0.Px1 \"In A.3 MaTTS Details ‣ Appendix A Experiment Details ‣ ReasoningBank: Scaling Agent Self-Evolving with Reasoning Memory\")\n",
    "        2. [Best-of-N Calculation Details.](https://arxiv.org/html/2509.25140v1#A1.SS3.SSS0.Px2 \"In A.3 MaTTS Details ‣ Appendix A Experiment Details ‣ ReasoningBank: Scaling Agent Self-Evolving with Reasoning Memory\")\n",
    "  9. [B Details for Experiment Settings](https://arxiv.org/html/2509.25140v1#A2 \"In ReasoningBank: Scaling Agent Self-Evolving with Reasoning Memory\")\n",
    "     1. [B.1 Web Browsing](https://arxiv.org/html/2509.25140v1#A2.SS1 \"In Appendix B Details for Experiment Settings ‣ ReasoningBank: Scaling Agent Self-Evolving with Reasoning Memory\")\n",
    "     2. [B.2 Software Engineering](https://arxiv.org/html/2509.25140v1#A2.SS2 \"In Appendix B Details for Experiment Settings ‣ ReasoningBank: Scaling Agent Self-Evolving with Reasoning Memory\")\n",
    "        1. [B.2.1 Experiment Setup](https://arxiv.org/html/2509.25140v1#A2.SS2.SSS1 \"In B.2 Software Engineering ‣ Appendix B Details for Experiment Settings ‣ ReasoningBank: Scaling Agent Self-Evolving with Reasoning Memory\")\n",
    "  10. [C Additional Analyses](https://arxiv.org/html/2509.25140v1#A3 \"In ReasoningBank: Scaling Agent Self-Evolving with Reasoning Memory\")\n",
    "     1. [C.1 Number of Retrieved Experiences](https://arxiv.org/html/2509.25140v1#A3.SS1 \"In Appendix C Additional Analyses ‣ ReasoningBank: Scaling Agent Self-Evolving with Reasoning Memory\")\n",
    "     2. [C.2 Pass@k Analysis](https://arxiv.org/html/2509.25140v1#A3.SS2 \"In Appendix C Additional Analyses ‣ ReasoningBank: Scaling Agent Self-Evolving with Reasoning Memory\")\n",
    "        1. [Memory-aware scaling improves sample efficiency and sustains stronger performance gains.](https://arxiv.org/html/2509.25140v1#A3.SS2.SSS0.Px1 \"In C.2 Pass@k Analysis ‣ Appendix C Additional Analyses ‣ ReasoningBank: Scaling Agent Self-Evolving with Reasoning Memory\")\n",
    "     3. [C.3 Case Study](https://arxiv.org/html/2509.25140v1#A3.SS3 \"In Appendix C Additional Analyses ‣ ReasoningBank: Scaling Agent Self-Evolving with Reasoning Memory\")\n",
    "  11. [D Future Directions](https://arxiv.org/html/2509.25140v1#A4 \"In ReasoningBank: Scaling Agent Self-Evolving with Reasoning Memory\")\n",
    "  12. [E Limitations](https://arxiv.org/html/2509.25140v1#A5 \"In ReasoningBank: Scaling Agent Self-Evolving with Reasoning Memory\")\n",
    "\n",
    "\n",
    "\n",
    "\\pdftrailerid\n",
    "\n",
    "redacted\\correspondingauthorsiruo2@illinois.edu, {junyann, chenyulee}@google.com\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c0438a",
   "metadata": {},
   "source": [
    "#  ReasoningBank: Scaling Agent Self-Evolving with Reasoning Memory\n",
    "\n",
    "Siru Ouyang  Jun Yan  Google Cloud AI Research  I-Hung Hsu  Google Cloud AI Research  Yanfei Chen  Google Cloud AI Research  Ke Jiang  Google Cloud AI Research  Zifeng Wang  Google Cloud AI Research  Rujun Han  Google Cloud AI Research  Long T. Le  Google Cloud AI Research  Samira Daruki  Google Cloud AI Research  Xiangru Tang  Yale University  Vishy Tirumalashetty  Google Cloud AI Research  George Lee  Google Cloud AI Research  Mahsan Rofouei  Google Cloud AI  Hangfei Lin  Google Cloud AI  Jiawei Han  University of Illinois Urbana-Champaign  Chen-Yu Lee  Google Cloud AI Research  Tomas Pfister  Google Cloud AI Research \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ccc749",
   "metadata": {},
   "source": [
    "###### Abstract\n",
    "\n",
    "With the growing adoption of large language model agents in persistent real-world roles, they naturally encounter continuous streams of tasks. A key limitation, however, is their failure to learn from the accumulated interaction history, forcing them to discard valuable insights and repeat past errors. We propose ReasoningBank, a novel memory framework that distills generalizable reasoning strategies from an agent’s self-judged successful and failed experiences. At test time, an agent retrieves relevant memories from ReasoningBank to inform its interaction and then integrates new learnings back, enabling it to become more capable over time. Building on this powerful experience learner, we further introduce memory-aware test-time scaling (MaTTS), which accelerates and diversifies this learning process by scaling up the agent’s interaction experience. By allocating more compute to each task, the agent generates abundant, diverse experiences that provide rich contrastive signals for synthesizing higher-quality memory. The better memory in turn guides more effective scaling, establishing a powerful synergy between memory and test-time scaling. Across web browsing and software engineering benchmarks, ReasoningBank consistently outperforms existing memory mechanisms that store raw trajectories or only successful task routines, improving both effectiveness and efficiency; MaTTS further amplifies these gains. These findings establish memory-driven experience scaling as a new scaling dimension, enabling agents to self-evolve with emergent behaviors naturally arise.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be6958f",
   "metadata": {},
   "source": [
    "##  1 Introduction\n",
    "\n",
    "The rapid advancement of large language models (LLMs) has significantly accelerated the development of LLM agents (Wang et al., [2024](https://arxiv.org/html/2509.25140v1#bib.bib44); Liu et al., [2025a](https://arxiv.org/html/2509.25140v1#bib.bib24)), which are crucial for tackling complex real-world tasks that require multi-step interactions with environments, including web browsing (Gur et al., [2024](https://arxiv.org/html/2509.25140v1#bib.bib13)) and computer use (Yang et al., [2024](https://arxiv.org/html/2509.25140v1#bib.bib53); Xie et al., [2024](https://arxiv.org/html/2509.25140v1#bib.bib51)). As these agents are increasingly deployed in persistent, long-running roles, they naturally encounter a continuous stream of tasks throughout their lifetime. However, they largely fail to learn from their accumulated experience across tasks. By approaching each task in isolation, they are doomed to repeat past errors (Yin et al., [2025](https://arxiv.org/html/2509.25140v1#bib.bib55)), discard valuable insights from related problems, and lack self-evolving capabilities that make the agent system more capable over time (Gao et al., [2025](https://arxiv.org/html/2509.25140v1#bib.bib11)). This highlights the necessity of building memory-aware agent systems that could learn from their past experiences (Zhang et al., [2024b](https://arxiv.org/html/2509.25140v1#bib.bib62)).\n",
    "\n",
    "Recent efforts on agent memory have primarily focused on storing past interactions for reuse (Tang et al., [2025b](https://arxiv.org/html/2509.25140v1#bib.bib42); Chen et al., [2025](https://arxiv.org/html/2509.25140v1#bib.bib2); Sun et al., [2025](https://arxiv.org/html/2509.25140v1#bib.bib39)). While useful, these approaches are often limited to leveraging raw trajectories (Zheng et al., [2024](https://arxiv.org/html/2509.25140v1#bib.bib64); Kagaya et al., [2024](https://arxiv.org/html/2509.25140v1#bib.bib18); Kong et al., [2025](https://arxiv.org/html/2509.25140v1#bib.bib19)) or common, successful routines (i.e., workflows, procedures) (Wang et al., [2025d](https://arxiv.org/html/2509.25140v1#bib.bib47); Fang et al., [2025](https://arxiv.org/html/2509.25140v1#bib.bib9)). These approaches suffer from two fundamental drawbacks. First, they lack the ability to distill higher-level, transferable reasoning patterns. Second, by over-emphasizing successful experiences, they leave the valuable lessons from an agent’s own failures largely underexplored (Zhang et al., [2024a](https://arxiv.org/html/2509.25140v1#bib.bib60)). Consequently, existing memory designs often remain limited to passive record-keeping rather than providing actionable, generalizable guidance for future decisions.\n",
    "\n",
    "![Refer to caption](https://arxiv.org/html/2509.25140v1/x1.png#ai) Figure 1: ReasoningBank induces reusable reasoning strategies, making memory items more transferrable for future use. This enables agents to continuously evolve and achieve higher accumulative success rates than the “No Memory” baseline on the WebArena-Admin subset. \n",
    "\n",
    "To bridge this gap, we propose ReasoningBank, a novel memory framework for agent systems. ReasoningBank distills and organizes memory items from both successful and failed experiences judged by the agent itself without ground-truth labels. As shown in Figure [1](https://arxiv.org/html/2509.25140v1#S1.F1 \"Figure 1 ‣ 1 Introduction ‣ ReasoningBank: Scaling Agent Self-Evolving with Reasoning Memory\"), it captures not only effective strategies from successes but also crucial preventative lessons from failures, abstracting them into a collection of actionable principles. This process operates in a closed loop: when facing a new task, the agent retrieves relevant memories from ReasoningBank to guide its actions. Afterward, the new experience is analyzed, distilled, and consolidated back into the ReasoningBank, allowing the agent to continuously evolve and improve its strategic capabilities.\n",
    "\n",
    "With ReasoningBank as a strong experience learner, we study experience scaling to establish a powerful synergy between memory and test-time scaling. Instead of scaling experience through breadth by adding more tasks, we focus on scaling experience through depth by tackling each single task with more exploration. We introduce memory-aware test-time scaling (MaTTS) in both parallel and sequential settings, which generates diverse exploration to provide contrastive signals, enabling ReasoningBank to synthesize more generalizable memories. It creates a synergy between memory and test-time scaling: high-quality memory steers the scaled exploration toward more promising paths, while the rich experiences generated forge even stronger memories. This positive feedback loop positions memory-driven experience scaling as a new scaling dimension for agents.\n",
    "\n",
    "We conduct extensive experiments on challenging benchmarks for web browsing (WebArena, Mind2Web) and software engineering (SWE-Bench-Verified). We demonstrate that our approaches outperform baselines in both effectiveness (up to 34.2% relative improvement, Figure [4](https://arxiv.org/html/2509.25140v1#S4.F4 \"Figure 4 ‣ 4.3 Results of MaTTS ‣ 4 Experiments ‣ ReasoningBank: Scaling Agent Self-Evolving with Reasoning Memory\")(b)) and efficiency (16.0% less interaction steps, Table [1](https://arxiv.org/html/2509.25140v1#S4.T1 \"Table 1 ‣ 4 Experiments ‣ ReasoningBank: Scaling Agent Self-Evolving with Reasoning Memory\")). Specifically, ReasoningBank synergizes best with MaTTS, making it an essential component for memory-driven experience scaling.\n",
    "\n",
    "Our contributions are threefold: (1) We propose ReasoningBank, a novel memory framework that distills generalizable reasoning strategies from both successful and failed experiences, beyond prior work limited to raw trajectories or success-only routines. (2) We introduce MaTTS that creates a powerful synergy between memory and test-time scaling, establishing memory-driven experience as a new scaling dimension for agents. (3) We demonstrate through extensive experiments that our approaches not only improve effectiveness and efficiency over existing methods, but also enable agents to learn from failures and develop increasingly complex, emergent reasoning strategies over time.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc9a104",
   "metadata": {},
   "source": [
    "##  2 Related Work\n",
    "\n",
    "Memory for LLM Agents. Memory has emerged as an essential module in modern agent systems (Zhang et al., [2024b](https://arxiv.org/html/2509.25140v1#bib.bib62)) to enhance their performance by utilizing past information (Zhang et al., [2024b](https://arxiv.org/html/2509.25140v1#bib.bib62)). Existing memory systems organize and store information in various forms, including plain text (Packer et al., [2023](https://arxiv.org/html/2509.25140v1#bib.bib31)), latent knowledge embeddings (Wang et al., [2025b](https://arxiv.org/html/2509.25140v1#bib.bib45)) and structured graphs (Xu et al., [2025](https://arxiv.org/html/2509.25140v1#bib.bib52); Chhikara et al., [2025](https://arxiv.org/html/2509.25140v1#bib.bib4); Li et al., [2025b](https://arxiv.org/html/2509.25140v1#bib.bib22)). Beyond memory content, those methods usually involve retrieval mechanisms (e.g., semantic search) with memory management strategies (e.g., updating) (Tan et al., [2025](https://arxiv.org/html/2509.25140v1#bib.bib40); Hu et al., [2025a](https://arxiv.org/html/2509.25140v1#bib.bib14)). More recently, with the growing development of reinforcement learning (RL) in LLM agents, RL has also been leveraged for memory management in agent systems (Yu et al., [2025a](https://arxiv.org/html/2509.25140v1#bib.bib56); Zhou et al., [2025](https://arxiv.org/html/2509.25140v1#bib.bib67)). While most efforts primarily emphasizing personalization (Zhang et al., [2025](https://arxiv.org/html/2509.25140v1#bib.bib61); Zhong et al., [2024](https://arxiv.org/html/2509.25140v1#bib.bib65)) and long-context management (Maharana et al., [2024](https://arxiv.org/html/2509.25140v1#bib.bib28); Wu et al., [2025](https://arxiv.org/html/2509.25140v1#bib.bib49); Hu et al., [2025b](https://arxiv.org/html/2509.25140v1#bib.bib15)), this paper falls in the research line of learning from past experiences (Zhao et al., [2024](https://arxiv.org/html/2509.25140v1#bib.bib63); SU et al., [2025](https://arxiv.org/html/2509.25140v1#bib.bib38)) as memory, which is a critical aspect for developing self-evolving agent systems (Gao et al., [2025](https://arxiv.org/html/2509.25140v1#bib.bib11); Liang et al., [2024](https://arxiv.org/html/2509.25140v1#bib.bib23)). Different from previous works that emphasize reusing successful trajectories (Zheng et al., [2024](https://arxiv.org/html/2509.25140v1#bib.bib64); Tang et al., [2025a](https://arxiv.org/html/2509.25140v1#bib.bib41)) or procedural workflows (Wang et al., [2025d](https://arxiv.org/html/2509.25140v1#bib.bib47); Qian et al., [2024](https://arxiv.org/html/2509.25140v1#bib.bib33); Fang et al., [2025](https://arxiv.org/html/2509.25140v1#bib.bib9); Liu et al., [2025b](https://arxiv.org/html/2509.25140v1#bib.bib25)), ReasoningBank stores high-level strategies and reasoning hints. By abstracting experiences into reusable reasoning units, ReasoningBank enables agents to generalize not only from successful cases but also by learning from failures, thereby providing richer guidance for test-time learning. Additionally, we are the first to explore memory-aware test-time scaling, where ReasoningBank synergistically work with diverse signals from abundant exploration trajectories.\n",
    "\n",
    "Agent Test-Time Scaling. Test-time scaling (TTS) (Snell et al., [2025](https://arxiv.org/html/2509.25140v1#bib.bib37)) has demonstrated strong effectiveness and has become a widely adopted practice in end-to-end problem-solving such as coding (Li et al., [2025a](https://arxiv.org/html/2509.25140v1#bib.bib21); Yu et al., [2025c](https://arxiv.org/html/2509.25140v1#bib.bib58)) and math reasoning (Muennighoff et al., [2025](https://arxiv.org/html/2509.25140v1#bib.bib30)), where methods including best-of-N (Chow et al., [2025](https://arxiv.org/html/2509.25140v1#bib.bib5)), beam search (Wu et al., [2024b](https://arxiv.org/html/2509.25140v1#bib.bib50)), and leveraging verifiers (Setlur et al., [2025](https://arxiv.org/html/2509.25140v1#bib.bib34)) are commonly employed. However, its application to multi-turn interactive scenarios, particularly agentic tasks, remains underexplored. Existing works mainly adapt the lesson learned from reasoning tasks (Zhu et al., [2025b](https://arxiv.org/html/2509.25140v1#bib.bib69)) and scale different dimensions of agentic systems, including the search space for each action (Yu et al., [2025b](https://arxiv.org/html/2509.25140v1#bib.bib57)), the number of agents in multi-agent systems (Jin et al., [2025](https://arxiv.org/html/2509.25140v1#bib.bib17)), and the number of interactions with the environment (Shen et al., [2025](https://arxiv.org/html/2509.25140v1#bib.bib36)). We found that none of these efforts considers the role of agent memory in scaling, where an agent can learn from past experiences to guide future decisions. Our work extends this line of research by introducing memory-aware test-time scaling (MaTTS). As we will show in our empirical results (§[4.3](https://arxiv.org/html/2509.25140v1#S4.SS3 \"4.3 Results of MaTTS ‣ 4 Experiments ‣ ReasoningBank: Scaling Agent Self-Evolving with Reasoning Memory\") and §[4.4](https://arxiv.org/html/2509.25140v1#S4.SS4 \"4.4 Synergy of Memory and Test-Time Scaling ‣ 4 Experiments ‣ ReasoningBank: Scaling Agent Self-Evolving with Reasoning Memory\")), memory offers benefits beyond mere computational scaling, where memory and scaling synergistically work towards better performance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339d39c2",
   "metadata": {},
   "source": [
    "##  3 Methodology\n",
    "\n",
    "In this section, we introduce the problem setup (§[3.1](https://arxiv.org/html/2509.25140v1#S3.SS1 \"3.1 Problem Formulation ‣ 3 Methodology ‣ ReasoningBank: Scaling Agent Self-Evolving with Reasoning Memory\")), and present our proposed ReasoningBank (§[3.2](https://arxiv.org/html/2509.25140v1#S3.SS2 \"3.2 ReasoningBank ‣ 3 Methodology ‣ ReasoningBank: Scaling Agent Self-Evolving with Reasoning Memory\")), based on which we further develop memory-aware test-time scaling (MaTTS) (§[3.3](https://arxiv.org/html/2509.25140v1#S3.SS3 \"3.3 MaTTS: Memory-aware Test-Time Scaling ‣ 3 Methodology ‣ ReasoningBank: Scaling Agent Self-Evolving with Reasoning Memory\")).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8725e88e",
   "metadata": {},
   "source": [
    "###  3.1 Problem Formulation\n",
    "\n",
    "Agent Configuration. The scope of this work focuses on LLM-based agents. The agent policy \\(\\pi_{\\mathcal{L}}(\\cdot|\\mathcal{M},\\mathcal{A})\\) is parameterized by the backbone LLM \\(\\mathcal{L}\\), conditioned on a memory module \\(\\mathcal{M}\\), and the action space \\(\\mathcal{A}\\), denoted as \\(\\pi_{\\mathcal{L}}\\) for short. The agent needs to perform a task via interacting with the environment, which can be viewed as a sequential decision-making process. Formally, the transition function of the environment is defined as \\(\\mathcal{T}(s_{t+1}|s_{t},a_{t})\\) where \\(s_{t}\\) is the state and \\(a_{t}\\) is the action selected by \\(\\pi_{\\mathcal{L}}\\) at time \\(t\\). We focus on web browsing and software engineering (SWE) tasks. \\(\\mathcal{A}\\) is a set of web navigation operations for web browsing and bash commands for SWE tasks, \\(\\mathcal{M}\\) is ReasoningBank and initialized as empty. For each given task, the agent generates a trajectory of \\((o_{0:t},a_{0:t})\\) for \\(t\\) steps, where observation \\(o_{t}\\) is from the current state \\(s_{t}\\). Observations are text-based accessibility tree of web pages111We use the thinking process of \\(\\pi_{\\mathcal{L}}\\) as the approximation of \\(o_{0:t}\\) due to lengthy observation representations following Wang et al. ([2025d](https://arxiv.org/html/2509.25140v1#bib.bib47)). for web browsing tasks and code snippets for SWE. The agent needs to generate an action \\(a_{t+1}\\in\\mathcal{A}\\) via \\(\\pi_{\\mathcal{L}}(o_{0:t},a_{0:t};\\mathcal{M},\\mathcal{A})\\rightarrow a_{t+1}\\). For implementation, the memory module \\(\\mathcal{M}\\) contributes relevant memories as additional system instruction for \\(\\pi_{\\mathcal{L}}\\).\n",
    "\n",
    "Test-Time Learning. We focus on the test-time learning paradigm (Wu et al., [2024a](https://arxiv.org/html/2509.25140v1#bib.bib48); Wang et al., [2025c](https://arxiv.org/html/2509.25140v1#bib.bib46)) where a sequence of task queries \\(\\mathcal{Q}=\\\\{q_{1},q_{2},...,q_{N}\\\\}\\) arrives in a streaming fashion, i.e., each query is revealed and must be completed sequentially without access to future ones. In this setting, no ground truth is available during test-time, so the agent must continually evolve by only leveraging its own past trajectories and any self-verification without relying on external labels. This streaming setting highlights two key challenges: (i) how to extract and preserve useful memory from past trajectories, and (ii) how to effectively leverage such memory for future queries to avoid redundantly re-discovering already successful strategies or repeating past mistakes.\n",
    "\n",
    "![Refer to caption](https://arxiv.org/html/2509.25140v1/x2.png#ai) Figure 2: Overview of ReasoningBank. Experiences are distilled into structured memory items with a title, description, and content. For each new task, the agent retrieves relevant items to interact with the environment, and constructs new ones from both successful and failed trajectories. These items are then consolidated into ReasoningBank, forming a closed-loop memory process.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552f5c4f",
   "metadata": {},
   "source": [
    "###  3.2 ReasoningBank\n",
    "\n",
    "Past raw trajectories (or experiences), while being comprehensive and original, are often too lengthy and noisy to be directly applied to the current user query. As illustrated in Figure [2](https://arxiv.org/html/2509.25140v1#S3.F2 \"Figure 2 ‣ 3.1 Problem Formulation ‣ 3 Methodology ‣ ReasoningBank: Scaling Agent Self-Evolving with Reasoning Memory\"), ReasoningBank distills useful strategies and reasoning hints from past experiences into structured memory items, which are then stored for future reuse.\n",
    "\n",
    "Memory Schema. Memory items in ReasoningBank are designed and induced from past experiences as structured knowledge units that abstract away low-level execution details while preserving transferrable reasoning patterns and strategies. Each memory item specifies three components: (i) a title, which serves as a concise identifier summarizing the core strategy or reasoning pattern; (ii) a description, which provides a brief one-sentence summary of the memory item; and (iii) the content, which records the distilled reasoning steps, decision rationales, or operational insights extracted from past experiences. Together, memory items extracted are both human-interpretable and machine-usable, facilitating efficient usage and integration with agents.\n",
    "\n",
    "Integration of ReasoningBank with Agents. An agent equipped with ReasoningBank can draw upon a curated pool of transferable strategies to guide decision-making. This enables the agent to recall effective insights, avoid previously observed pitfalls, and adapt more robustly to unseen queries. The integration proceeds in three steps: (i) memory retrieval, (ii) memory construction, and (iii) memory consolidation, as shown in Figure [2](https://arxiv.org/html/2509.25140v1#S3.F2 \"Figure 2 ‣ 3.1 Problem Formulation ‣ 3 Methodology ‣ ReasoningBank: Scaling Agent Self-Evolving with Reasoning Memory\"). During memory retrieval, the agent queries ReasoningBank with the current query context to identify the top-\\(k\\) relevant experiences and their corresponding memory items using embedding-based similarity search. Retrieved items are injected into the agent’s system instruction, ensuring that the decision-making is grounded with useful past experiences. When the current query task is completed, we will perform memory construction to extract new memory items. The first step is to obtain proxy signals for the correctness of completed trajectories: we adopt an LLM-as-a-judge (Gu et al., [2024](https://arxiv.org/html/2509.25140v1#bib.bib12)) to label outcomes as success or failure given the query and trajectory, without access to any ground-truth. Based on these signals, we apply different extraction strategies: successful experiences contribute validated strategies, while failed ones supply counterfactual signals and pitfalls that help sharpen guardrails. In practice, we extract multiple memory items for each trajectory/experience as detailed in Appendix [A.1](https://arxiv.org/html/2509.25140v1#A1.SS1 \"A.1 Prompts Used for ReasoningBank ‣ Appendix A Experiment Details ‣ ReasoningBank: Scaling Agent Self-Evolving with Reasoning Memory\"). Finally, memory consolidation incorporates these items into ReasoningBank with a simple addition operation, maintaining an evolving repository of memory items. Details are in Appendix [A.2](https://arxiv.org/html/2509.25140v1#A1.SS2 \"A.2 Implementation Details ‣ Appendix A Experiment Details ‣ ReasoningBank: Scaling Agent Self-Evolving with Reasoning Memory\"). Together, these steps form a closed-loop process: the agent leverages past experiences, constructs new memory from current tasks, and continually updates its memory, enabling sustained evolvement in test-time learning scenarios.222We deliberately keep the memory usage pipeline simple, avoiding additional complexity in retrieval or consolidation so as to highlight the contribution of ReasoningBank itself. These components, however, can be further enhanced with more sophisticated techniques, which could provide additional benefits.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137923aa",
   "metadata": {},
   "source": [
    "###  3.3 MaTTS: Memory-aware Test-Time Scaling\n",
    "\n",
    "ReasoningBank enables learning from experiences to translate more experiences into greater improvements. As test-time scaling (Snell et al., [2025](https://arxiv.org/html/2509.25140v1#bib.bib37)) recently emerged as a powerful strategy for boosting the performance of LLM agents (Zhu et al., [2025a](https://arxiv.org/html/2509.25140v1#bib.bib68)), it shows strong potential by allocating additional inference-time computation to generate abundant exploration histories. A direct combination of ReasoningBank and test-time scaling is depicted in Figure [3](https://arxiv.org/html/2509.25140v1#S3.F3 \"Figure 3 ‣ 3.3 MaTTS: Memory-aware Test-Time Scaling ‣ 3 Methodology ‣ ReasoningBank: Scaling Agent Self-Evolving with Reasoning Memory\")(a), where more trajectories are independently converted to more memory items. However, this vanilla form is suboptimal because it does not leverage inherent contrastive signal that arises from redundant exploration on the same problem, which limits the resulting performance advantage brought by test-time scaling. To address this, we propose Memory-aware Test-Time Scaling (MaTTS), a novel integration of test-time scaling with ReasoningBank. Unlike the vanilla approach, MaTTS deliberately learns from the abundant successful and failure trajectories generated during scaling for more effective memory curation. We design two complementary instantiations for MaTTS, parallel scaling and sequential scaling, as illustrated in Figure [3](https://arxiv.org/html/2509.25140v1#S3.F3 \"Figure 3 ‣ 3.3 MaTTS: Memory-aware Test-Time Scaling ‣ 3 Methodology ‣ ReasoningBank: Scaling Agent Self-Evolving with Reasoning Memory\")(b) and [3](https://arxiv.org/html/2509.25140v1#S3.F3 \"Figure 3 ‣ 3.3 MaTTS: Memory-aware Test-Time Scaling ‣ 3 Methodology ‣ ReasoningBank: Scaling Agent Self-Evolving with Reasoning Memory\")(c) with detailed implementation in Appendix [A.3](https://arxiv.org/html/2509.25140v1#A1.SS3 \"A.3 MaTTS Details ‣ Appendix A Experiment Details ‣ ReasoningBank: Scaling Agent Self-Evolving with Reasoning Memory\").\n",
    "\n",
    "![Refer to caption](https://arxiv.org/html/2509.25140v1/x3.png#ai) Figure 3: Comparison of (a) vanilla TTS and MaTTS with (b) parallel scaling, where self-contrast across multiple trajectories curates reliable memory, and (c) sequential scaling, where self-refinement enriches memory with intermediate reasoning signals.\n",
    "\n",
    "Parallel Scaling. In the parallel setting, we generate multiple trajectories for the same query under the guidance of retrieved memory items. By comparing and contrasting (self-contrast (Chen et al., [2020](https://arxiv.org/html/2509.25140v1#bib.bib3))) across different trajectories, the agent can identify consistent reasoning patterns while filtering out spurious solutions. This process enables more reliable memory curation from multiple trials of a single query that promotes diverse exploration.\n",
    "\n",
    "Sequential Scaling. We iteratively refines its reasoning within a single trajectory after the initial completion, following the principle of self-refinement (Madaan et al., [2023](https://arxiv.org/html/2509.25140v1#bib.bib27)). During this process, the intermediate notes generated in self-refinement are also used as valuable signals for memory, since they capture reasoning attempts, corrections, and insights that may not appear in the final solution.\n",
    "\n",
    "We define the scaling factor \\(k\\), denoting the number of trajectories for parallel scaling and refinement steps for sequential scaling. Equipped with ReasoningBank, both parallel and sequential strategies become memory-aware, ensuring that the additional computation allocated at test time translates into more transferable and higher-quality memory for future tasks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28378840",
   "metadata": {},
   "source": [
    "##  4 Experiments\n",
    "\n",
    "Table 1: Experiment results of ReasoningBank on WebArena benchmark. Success rate (SR \\(\\uparrow\\)) and the number of steps (Step \\(\\downarrow\\)) are reported on 5 subsets for 3 different backbone LLMs. Models | Shopping | Admin | Gitlab | Reddit | Multi | Overall  \n",
    "---|---|---|---|---|---|---  \n",
    "(187) | (182) | (180) | (106) | (29) | (684)  \n",
    "| SR | Step | SR | Step | SR | Step | SR | Step | SR | Step | SR | Step  \n",
    "Gemini-2.5-flash  \n",
    "No Memory | 39.0 | 8.2 | 44.5 | 9.5 | 33.9 | 13.3 | 55.7 | 6.7 | 10.3 | 10.0 | 40.5 | 9.7  \n",
    "Synapse | 40.6 | 7.0 | 45.1 | 9.1 | 35.6 | 13.0 | 59.4 | 6.5 | 10.3 | 10.5 | 42.1 | 9.2  \n",
    "AWM | 44.4 | 7.0 | 46.7 | 8.8 | 37.2 | 13.2 | 62.3 | 6.1 | 3.4 | 7.7 | 44.1 | 9.0  \n",
    "ReasoningBank | 49.7 | 6.1 | 51.1 | 8.2 | 40.6 | 12.3 | 67.0 | 5.6 | 13.8 | 8.8 | 48.8 | 8.3  \n",
    "Gemini-2.5-pro  \n",
    "No Memory | 45.5 | 7.6 | 51.1 | 8.7 | 35.0 | 11.6 | 71.7 | 6.0 | 6.9 | 8.8 | 46.7 | 8.8  \n",
    "Synapse | 46.5 | 6.6 | 52.2 | 8.9 | 38.3 | 11.3 | 68.9 | 5.9 | 6.9 | 9.0 | 47.7 | 8.5  \n",
    "AWM | 48.1 | 6.4 | 49.3 | 9.8 | 40.0 | 11.2 | 68.9 | 6.4 | 3.4 | 9.3 | 47.6 | 8.7  \n",
    "ReasoningBank | 51.9 | 6.0 | 56.6 | 7.7 | 44.4 | 9.8 | 80.2 | 5.1 | 13.8 | 8.2 | 53.9 | 7.4  \n",
    "Claude-3.7-sonnet  \n",
    "No Memory | 38.5 | 6.1 | 49.5 | 8.4 | 36.7 | 10.6 | 53.8 | 5.5 | 0.0 | 11.6 | 41.7 | 8.0  \n",
    "Synapse | 39.6 | 5.8 | 50.5 | 8.5 | 38.0 | 10.0 | 53.8 | 6.1 | 0.0 | 11.8 | 42.6 | 7.9  \n",
    "AWM | 39.6 | 7.2 | 47.8 | 9.3 | 34.6 | 10.9 | 52.8 | 7.0 | 0.0 | 12.4 | 40.8 | 8.9  \n",
    "ReasoningBank | 44.9 | 5.6 | 53.3 | 7.6 | 41.1 | 9.5 | 57.5 | 5.2 | 3.4 | 10.5 | 46.3 | 7.3  \n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69cf253e",
   "metadata": {},
   "source": [
    "###  4.1 Setup\n",
    "\n",
    "Following existing work (Wang et al., [2025d](https://arxiv.org/html/2509.25140v1#bib.bib47)), we conduct experiments on WebArena (Zhou et al., [2024](https://arxiv.org/html/2509.25140v1#bib.bib66)) which features general web navigation across diverse domains,333We exclude the domain of Map due to website issues following Miyai et al. ([2025](https://arxiv.org/html/2509.25140v1#bib.bib29)) for a fair comparison. and Mind2Web (Deng et al., [2023](https://arxiv.org/html/2509.25140v1#bib.bib8)) that tests generalization of agents on versatile operations and environments. We also conduct experiment on SWE-Bench-Verified (Jimenez et al., [2024](https://arxiv.org/html/2509.25140v1#bib.bib16)) for repository-level issue-resolving. For comparison, we consider baselines ranging from memory-free agents (No Memory) to trajectory-based memory (Synapse) (Zheng et al., [2024](https://arxiv.org/html/2509.25140v1#bib.bib64)) and workflow-based memory (AWM) (Wang et al., [2025d](https://arxiv.org/html/2509.25140v1#bib.bib47)). Our agents are built on Gemini-2.5 (Comanici et al., [2025](https://arxiv.org/html/2509.25140v1#bib.bib6)) and Claude-3.7 (Anthropic, [2025](https://arxiv.org/html/2509.25140v1#bib.bib1)) models in BrowserGym (de Chezelles et al., [2025](https://arxiv.org/html/2509.25140v1#bib.bib7)) environment for web browsing and bash-only environment for SWE, following ReAct (Yao et al., [2023](https://arxiv.org/html/2509.25140v1#bib.bib54)) style with default decoding configurations. Evaluation focuses on effectiveness (success rate) and efficiency (average interaction steps), with specific metrics varying for each dataset. Full descriptions for datasets, baselines, implementations, and evaluation protocols are in Appendix [B](https://arxiv.org/html/2509.25140v1#A2 \"Appendix B Details for Experiment Settings ‣ ReasoningBank: Scaling Agent Self-Evolving with Reasoning Memory\").\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d2aaad",
   "metadata": {},
   "source": [
    "###  4.2 Results of ReasoningBank\n",
    "\n",
    "Tables [1](https://arxiv.org/html/2509.25140v1#S4.T1 \"Table 1 ‣ 4 Experiments ‣ ReasoningBank: Scaling Agent Self-Evolving with Reasoning Memory\"), [2](https://arxiv.org/html/2509.25140v1#S4.T2 \"Table 2 ‣ 4.2 Results of ReasoningBank ‣ 4 Experiments ‣ ReasoningBank: Scaling Agent Self-Evolving with Reasoning Memory\"), [3](https://arxiv.org/html/2509.25140v1#S4.T3 \"Table 3 ‣ 4.2 Results of ReasoningBank ‣ 4 Experiments ‣ ReasoningBank: Scaling Agent Self-Evolving with Reasoning Memory\") summarize the main evaluation results of ReasoningBank on WebArena, Mind2Web, and SWE-Bench-Verified. We have the following observations.\n",
    "\n",
    "ReasoningBank consistently outperforms baselines across LLM backbones on all datasets. Specifically, ReasoningBank improves the overall success rate on WebArena (Table [1](https://arxiv.org/html/2509.25140v1#S4.T1 \"Table 1 ‣ 4 Experiments ‣ ReasoningBank: Scaling Agent Self-Evolving with Reasoning Memory\")) by \\(+8.3\\), \\(+7.2\\), and \\(+4.6\\) with three different backbone LLMs compared to memory-free agents. A similar pattern holds on Mind2Web (Table [3](https://arxiv.org/html/2509.25140v1#S4.T3 \"Table 3 ‣ 4.2 Results of ReasoningBank ‣ 4 Experiments ‣ ReasoningBank: Scaling Agent Self-Evolving with Reasoning Memory\")), where ReasoningBank delivers clear gains across cross-task, cross-website, and cross-domain settings, underscoring both the consistency and scalability of its benefits across datasets and model sizes. Results on SWE-Bench-Verified (Table [2](https://arxiv.org/html/2509.25140v1#S4.T2 \"Table 2 ‣ 4.2 Results of ReasoningBank ‣ 4 Experiments ‣ ReasoningBank: Scaling Agent Self-Evolving with Reasoning Memory\")) further confirm its robustness. Crucially, unlike baselines such as Synapse and AWM that rely on a narrow, homogeneous memory source derived exclusively from successful trajectories, ReasoningBank employs a superior extraction strategy that is key to its consistent outperformance.\n",
    "\n",
    "Table 2: Experiment results of ReasoningBank on SWE-Bench-Verified dataset for issue-resolving in a given repository. Methods | Resolve Rate | Step  \n",
    "---|---|---  \n",
    "Gemini-2.5-flash  \n",
    "No Memory | 34.2 | 30.3  \n",
    "Synapse | 35.4 | 30.7  \n",
    "ReasoningBank | 38.8 | 27.5  \n",
    "Gemini-2.5-pro  \n",
    "No Memory | 54.0 | 21.1  \n",
    "Synapse | 53.4 | 21.0  \n",
    "ReasoningBank | 57.4 | 19.8  \n",
    "  \n",
    "ReasoningBank enhances generalization with better transferrable memory across tasks. We also evaluate in challenging generalization settings. On WebArena (Table [1](https://arxiv.org/html/2509.25140v1#S4.T1 \"Table 1 ‣ 4 Experiments ‣ ReasoningBank: Scaling Agent Self-Evolving with Reasoning Memory\")), the Multi subset requires transferring memory across multiple websites, where ReasoningBank achieves a notable gain of \\(+4.6\\) averaged SR over the strongest baseline. In contrast, strong baselines such as AWM fail to provide gains and even degrade in this setting. On Mind2Web (Table [3](https://arxiv.org/html/2509.25140v1#S4.T3 \"Table 3 ‣ 4.2 Results of ReasoningBank ‣ 4 Experiments ‣ ReasoningBank: Scaling Agent Self-Evolving with Reasoning Memory\")), which includes cross-task, cross-website, and cross-domain evaluations that impose progressively higher demands, ReasoningBank consistently improves success rates. The gains are especially pronounced in the cross-domain setting, which requires the highest level of generalization. These results demonstrate that memory curated by ReasoningBank is more robust and transferable, enabling agents to generalize effectively across diverse scenarios.\n",
    "\n",
    "ReasoningBank achieves superior efficiency by leveraging past experiences as memory. In addition to higher success rates, ReasoningBank also reduces the number of interaction steps needed to complete tasks, as shown in the Step metric of Table [1](https://arxiv.org/html/2509.25140v1#S4.T1 \"Table 1 ‣ 4 Experiments ‣ ReasoningBank: Scaling Agent Self-Evolving with Reasoning Memory\") and [2](https://arxiv.org/html/2509.25140v1#S4.T2 \"Table 2 ‣ 4.2 Results of ReasoningBank ‣ 4 Experiments ‣ ReasoningBank: Scaling Agent Self-Evolving with Reasoning Memory\"). On WebArena, across almost all subsets and backbones, ReasoningBank lowers the average step count by up to \\(1.4\\) compared with “No Memory”, and \\(1.6\\) compared with other memory baselines. The average step on SWE-Bench-Verified is also smaller by saving \\(2.8\\) and \\(1.3\\) steps respectively. This indicates that ReasoningBank enables agents to solve tasks more efficiently by reusing and refining reasoning knowledge, thus avoiding unnecessary or redundant exploration.\n",
    "\n",
    "Table 3: Results on Mind2Web benchmark for cross-task, cross-website, and cross-domain generalization test. EA (\\(\\uparrow\\)) is short for element accuracy, AF1 (\\(\\uparrow\\)) is short for action F1, and SSR (\\(\\uparrow\\)) is short for step success rate. SR (\\(\\uparrow\\)) is the task-level success rate measuring if all steps are correct.  Models | Cross-Task | Cross-Website | Cross-Domain  \n",
    "---|---|---|---  \n",
    "(252) | (177) | (912)  \n",
    "| EA | AF1 | SSR | SR | EA | AF1 | SSR | SR | EA | AF1 | SSR | SR  \n",
    "Gemini-2.5-flash  \n",
    "No Memory | 46.0 | 59.1 | 40.3 | 3.3 | 39.8 | 45.1 | 31.7 | 1.7 | 35.8 | 37.9 | 31.9 | 1.0  \n",
    "Synapse | 47.0 | 59.5 | 41.2 | 3.5 | 40.3 | 46.0 | 32.1 | 1.9 | 36.3 | 38.5 | 32.4 | 1.1  \n",
    "AWM | 46.3 | 56.1 | 41.0 | 3.5 | 39.1 | 42.2 | 31.7 | 2.1 | 33.3 | 36.5 | 30.1 | 0.7  \n",
    "ReasoningBank | 52.1 | 60.4 | 44.9 | 4.8 | 44.3 | 52.6 | 33.9 | 2.3 | 40.6 | 41.3 | 36.6 | 1.6  \n",
    "Gemini-2.5-pro  \n",
    "No Memory | 49.3 | 60.2 | 44.4 | 3.5 | 41.2 | 49.8 | 34.8 | 3.4 | 37.9 | 37.7 | 35.0 | 1.4  \n",
    "Synapse | 50.1 | 61.0 | 44.7 | 3.6 | 41.8 | 51.2 | 35.0 | 3.2 | 38.5 | 39.8 | 35.6 | 1.5  \n",
    "AWM | 48.6 | 61.2 | 44.4 | 3.7 | 41.9 | 47.9 | 34.8 | 2.3 | 37.3 | 38.1 | 34.4 | 1.2  \n",
    "ReasoningBank | 53.6 | 62.7 | 45.6 | 5.1 | 46.1 | 54.8 | 36.9 | 3.8 | 42.8 | 45.2 | 38.1 | 1.7  \n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e602cb5a",
   "metadata": {},
   "source": [
    "###  4.3 Results of MaTTS\n",
    "\n",
    "We experimented MaTTS with Gemini-2.5-flash on Webarena-Shopping subset. By default, MaTTS integrates ReasoningBank, but it could also use other memory mechanisms. To investigate the overall scaling effect, we benchmark with (i) MaTTS w/o memory, which represents the scaling setting without memory mechanism, (ii) MaTTS w/o aggregation, which is equal to Vanilla TTS in Figure [3](https://arxiv.org/html/2509.25140v1#S3.F3 \"Figure 3 ‣ 3.3 MaTTS: Memory-aware Test-Time Scaling ‣ 3 Methodology ‣ ReasoningBank: Scaling Agent Self-Evolving with Reasoning Memory\")(a) and (iii) MaTTS to demonstrate the effect with respect to scaling factor \\(k\\). Notably, \\(k=1\\) is the setting without scaling. For parallel scaling, we compute Best-of-N (BoN) as the final metric detailed in Appendix [A.3](https://arxiv.org/html/2509.25140v1#A1.SS3 \"A.3 MaTTS Details ‣ Appendix A Experiment Details ‣ ReasoningBank: Scaling Agent Self-Evolving with Reasoning Memory\"). Results are shown in Figure [4](https://arxiv.org/html/2509.25140v1#S4.F4 \"Figure 4 ‣ 4.3 Results of MaTTS ‣ 4 Experiments ‣ ReasoningBank: Scaling Agent Self-Evolving with Reasoning Memory\").\n",
    "\n",
    "![Refer to caption](https://arxiv.org/html/2509.25140v1/x4.png#ai) Figure 4: Effect of scaling factor \\(k\\) for MaTTS under with ReasoningBank on WebArena-Shopping subset. We compare (a) parallel and (b) sequential test-time scaling.\n",
    "\n",
    "Both parallel scaling and sequential scaling boost performance. Increasing \\(k\\) generally improves success rate, confirming the benefit of allocating more inference-time computation. With MaTTS, parallel scaling grows from \\(49.7\\) (\\(k=1\\)) to \\(55.1\\) (\\(k=5\\)), while sequential scaling rises from \\(49.7\\) to \\(54.5\\). For the baseline of MaTTS w/o memory, the gains are smaller and less consistent (e.g., parallel scaling fluctuates between \\(39.0\\) and \\(42.2\\), sequential between \\(37.4\\) and \\(40.6\\)). In contrast, MaTTS enables stronger and more stable improvements across both scaling strategies, highlighting its role in making scaling more effective.\n",
    "\n",
    "MaTTS is consistently better than vanilla TTS. With ReasoningBank, MaTTS consistently surpasses MaTTS w/o aggregation (vanilla TTS), showing that memory-aware coordination and aggregation is important. Specifically, at \\(k=5\\), MaTTS achieves \\(55.1\\) in parallel scaling compared with \\(52.4\\) for vanilla TTS, and \\(54.5\\) versus \\(51.9\\) in sequential scaling. These improvements highlight that memory-aware scaling effectively directs the agent toward more promising solutions by synthesizing insights from multiple trajectories or interaction steps to leverage contrastive signals.\n",
    "\n",
    "Sequential scaling shows short-term advantage, but parallel dominates at larger scales for ReasoningBank. With stronger memory mechanisms such as ReasoningBank, sequential refinement brings higher gains at small \\(k\\), but its benefit quickly saturates—once the model either succeeds or fails decisively, further refinements add little new insight. In contrast, parallel scaling continues to provide diverse rollouts that allow the model to critique and improve upon its own generations, leading it to surpass sequential at larger \\(k\\) (e.g., \\(55.1\\) vs. \\(54.5\\) at \\(k=5\\)). In contrast, for vanilla TTS which is not equipped with memory module, sequential scaling yields little or even no benefit as scaling goes on, and parallel scaling consistently dominates.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc13fcda",
   "metadata": {},
   "source": [
    "###  4.4 Synergy of Memory and Test-Time Scaling\n",
    "\n",
    "While the previous section establishes the overall effectiveness of MaTTS, we highlight the synergy between memory and TTS in this section. Figure [5](https://arxiv.org/html/2509.25140v1#S4.F5 \"Figure 5 ‣ 4.4 Synergy of Memory and Test-Time Scaling ‣ 4 Experiments ‣ ReasoningBank: Scaling Agent Self-Evolving with Reasoning Memory\") presents a snapshot of MaTTS on the WebArena-Shopping subset with parallel scaling factor \\(k=3\\), where we report both Pass@1 (randomly selected trajectory) and Best-of-3 (BoN). This setting allows us to examine the bidirectional interaction between memory quality and scaling effectiveness.\n",
    "\n",
    "Better memory enables stronger test-time scaling performance. To see how memory improves the effectiveness of scaling, we focus on the BoN results, which directly measures an agent’s ability to surface the best outcome among multiple rollouts. As shown by blue bars in Figure [5](https://arxiv.org/html/2509.25140v1#S4.F5 \"Figure 5 ‣ 4.4 Synergy of Memory and Test-Time Scaling ‣ 4 Experiments ‣ ReasoningBank: Scaling Agent Self-Evolving with Reasoning Memory\"), the benefit of scaling depends critically on the underlying memory. Without memory, scaling yields slight improvement, with BoN rises only from \\(39.0\\) to \\(40.6\\). Weaker memory mechanisms such as Synapse and AWM provide moderate gains, reaching \\(42.8\\) and \\(45.5\\), respectively. In contrast, MaTTS with ReasoningBank delivers the strongest benefit, with BoN climbing from \\(49.7\\) to \\(52.4\\). These results show that high-quality memory directs scaling toward more promising rollouts, ensuring that the additional trajectories are not wasted but converted into higher success rates.\n",
    "\n",
    "![Refer to caption](https://arxiv.org/html/2509.25140v1/x5.png#ai) Figure 5: Snapshot of MaTTS on WebArena-Shopping subset with different memory mechanisms with \\(k=3\\). We compute BoN for all \\(3\\) trajectories and Pass@1 with one randomly selected trajectory.\n",
    "\n",
    "Scaling yields better memory curation. To fairly evaluate how scaling feeds back into memory, we report Pass@1, which measures the average quality of trajectories after memory curation and allows direct comparison with the no-scaling case. The trend is depicted in pink bars and is striking: scaling actually reduces performance for weaker memories, where Synapse drops from \\(40.6\\) to \\(40.1\\), and AWM from \\(44.4\\) to \\(41.2\\). These declines suggest that without strong guidance, the extra rollouts generated by scaling introduce noise rather than useful signals. In contrast, ReasoningBank is the only method that benefits: Pass@1 rises from \\(49.7\\) to \\(50.8\\), showing that high-quality memory can harness the diversity of scaling to extract constructive contrastive signals. This asymmetry highlights that scaling alone is insufficient; only when paired with good memory mechanism does it contribute to curation of more effective memory, thereby closing the virtuous cycle.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609c00b6",
   "metadata": {},
   "source": [
    "##  5 Analysis\n",
    "\n",
    "We analyze ReasoningBank beyond overall benchmark performance through three aspects: incorporating failure trajectories, examining emergent strategies, and evaluating efficiency across both successful and failed cases. Additional analyses are in Appendix [C](https://arxiv.org/html/2509.25140v1#A3 \"Appendix C Additional Analyses ‣ ReasoningBank: Scaling Agent Self-Evolving with Reasoning Memory\").\n",
    "\n",
    "![Refer to caption](https://arxiv.org/html/2509.25140v1/x6.png#ai) Figure 6: A case study illustrating emergent behaviors in ReasoningBank through memory items.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8471a5d7",
   "metadata": {},
   "source": [
    "###  5.1 Emergent behaviors with ReasoningBank\n",
    "\n",
    "We find that the strategies in ReasoningBank are not flat or monolithic, but instead evolve over time, exhibiting emergent behaviors that resemble the learning dynamics of RL (Wang et al., [2025a](https://arxiv.org/html/2509.25140v1#bib.bib43)). As illustrated in Figure [6](https://arxiv.org/html/2509.25140v1#S5.F6 \"Figure 6 ‣ 5 Analysis ‣ ReasoningBank: Scaling Agent Self-Evolving with Reasoning Memory\"), a memory item in ReasoningBank could gradually evolve during test-time learning process. It starts from execution-oriented or procedural strategies (e.g., find navigation links), where the agent follows straightforward action rules. It then progresses to adaptive self-reflections such as re-verifying identifiers to reduce simple mistakes. With more experiences, the same memory item evolves into adaptive checks, where the agent systematically leverages available search or filters to ensure completeness before results. Finally, it eventually matures into compositional strategies such as cross-referencing task requirements and reassessing options. This evolution highlights how ReasoningBank enables agents to refine strategies from low-level actions to high-level reasoning during test-time learning.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8d2e05",
   "metadata": {},
   "source": [
    "###  5.2 Incorporating failure trajectories\n",
    "\n",
    "![Refer to caption](https://arxiv.org/html/2509.25140v1/x7.png#ai) Figure 7: Ablation results of incorporating failure trajectories for memory induction.\n",
    "\n",
    "Figure [7](https://arxiv.org/html/2509.25140v1#S5.F7 \"Figure 7 ‣ 5.2 Incorporating failure trajectories ‣ 5 Analysis ‣ ReasoningBank: Scaling Agent Self-Evolving with Reasoning Memory\") compares different memory designs on WebArena-Shopping with Gemini-2.5-flash under two settings: using only successful trajectories versus leveraging both successes and failures. Baseline methods such as Synapse and AWM build memory solely from successful trajectories, and thus are not equipped to benefit from failures. As a result, when failures are added, their performance is limited or even degraded: Synapse increases only from \\(40.6\\) (success only) to \\(41.7\\) (with failures), while AWM drops from \\(44.4\\) to \\(42.2\\). In contrast, the design of ReasoningBank enables distillation of reasoning patterns from _both_ successes and failures, achieving \\(46.5\\) on success-only traces and further improving to \\(49.7\\) when failures are included. This highlights that, unlike baselines, ReasoningBank can transform failures into constructive signals rather than noise, enabling more robust generalization.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7d975d",
   "metadata": {},
   "source": [
    "###  5.3 Efficiency Study\n",
    "\n",
    "Table 4: Average number of steps on successful and failed test instances across four WebArena domains. ReasoningBank consistently reduces the number of steps compared to the vanilla baseline, with notably larger reductions on successful instances. Models | Shopping | Admin | Gitlab | Reddit  \n",
    "---|---|---|---|---  \n",
    "Successful | Failed | Successful | Failed | Successful | Failed | Successful | Failed  \n",
    "No Memory | 6.8 | 8.7 | 8.4 | 10.4 | 8.6 | 15.7 | 6.1 | 7.6  \n",
    "ReasoningBank |  4.7 \\(\\downarrow\\)2.1 |  7.3 \\(\\downarrow\\)1.4 |  7.0 \\(\\downarrow\\)1.4 |  9.5 \\(\\downarrow\\)0.9 |  7.6 \\(\\downarrow\\)1.0 |  15.5 \\(\\downarrow\\)0.2 |  5.0 \\(\\downarrow\\)1.1 |  6.8 \\(\\downarrow\\)0.8  \n",
    "  \n",
    "While the overall number of steps in Table [1](https://arxiv.org/html/2509.25140v1#S4.T1 \"Table 1 ‣ 4 Experiments ‣ ReasoningBank: Scaling Agent Self-Evolving with Reasoning Memory\") provides a general view of model efficiency, it does not distinguish whether reductions come from successful or failed trajectories. To gain deeper insight, we further separate the analysis into successful and failed test cases, which allows us to understand the source of step reduction: a desirable system should reduce unnecessary exploration when it is on the right track, rather than merely cutting short failed attempts. The results are shown in Table [4](https://arxiv.org/html/2509.25140v1#S5.T4 \"Table 4 ‣ 5.3 Efficiency Study ‣ 5 Analysis ‣ ReasoningBank: Scaling Agent Self-Evolving with Reasoning Memory\"). We find that ReasoningBank consistently reduces the number of steps across all domains compared to the baseline. More importantly, the reduction is particularly pronounced on successful cases, reaching up to \\(2.1\\) fewer steps (a \\(26.9\\%\\) relative reduction) than on failed ones. This indicates that ReasoningBank primarily helps the agent reach solutions with fewer interactions by strengthening its ability to follow effective reasoning paths rather than simply truncating failed trajectories, which highlight the role of memory in guiding purposeful decision-making and improving efficiency in practice.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7b196f",
   "metadata": {},
   "source": [
    "##  6 Conclusion\n",
    "\n",
    "We introduce ReasoningBank, a memory framework that distills strategy-level reasoning signals from both successes and failures and integrates them into test-time scaling (MaTTS). Extensive experiments show that ReasoningBank consistently improves performance while reducing redundant exploration. Further results reveal a strong synergy between memory and scaling: ReasoningBank guides scaling toward more promising rollouts, while diverse rollouts enrich memory with valuable contrastive signals. We also provide analyses of individual components and emergent behaviors. Our findings suggest a practical pathway toward building adaptive and lifelong-learning agents, with additional future directions and limitations in Appendix [D](https://arxiv.org/html/2509.25140v1#A4 \"Appendix D Future Directions ‣ ReasoningBank: Scaling Agent Self-Evolving with Reasoning Memory\") and [E](https://arxiv.org/html/2509.25140v1#A5 \"Appendix E Limitations ‣ ReasoningBank: Scaling Agent Self-Evolving with Reasoning Memory\").\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5837d4",
   "metadata": {},
   "source": [
    "##  7 Acknowledgments\n",
    "\n",
    "We thank Jiao Sun, Jing Nathan Yan, and members from Google Cloud AI Research for their valuable feedback during the preparation of the paper.\n",
    "\n",
    "\\nobibliography\n",
    "\n",
    "*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1beec53c",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "  * Anthropic (2025) Anthropic.  Claude 3.7 sonnet and claude code, 2025.  URL <https://www.anthropic.com/news/claude-3-7-sonnet>. \n",
    "  * Chen et al. (2025) S. Chen, S. Lin, X. Gu, Y. Shi, H. Lian, L. Yun, D. Chen, W. Sun, L. Cao, and Q. Wang.  Swe-exp: Experience-driven software issue resolution.  _ArXiv preprint_ , abs/2507.23361, 2025.  URL <https://arxiv.org/abs/2507.23361>. \n",
    "  * Chen et al. (2020) T. Chen, S. Kornblith, M. Norouzi, and G. E. Hinton.  A simple framework for contrastive learning of visual representations.  In _Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event_ , volume 119 of _Proceedings of Machine Learning Research_ , pages 1597–1607. PMLR, 2020.  URL <http://proceedings.mlr.press/v119/chen20j.html>. \n",
    "  * Chhikara et al. (2025) P. Chhikara, D. Khant, S. Aryan, T. Singh, and D. Yadav.  Mem0: Building production-ready ai agents with scalable long-term memory.  _ArXiv preprint_ , abs/2504.19413, 2025.  URL <https://arxiv.org/abs/2504.19413>. \n",
    "  * Chow et al. (2025) Y. Chow, G. Tennenholtz, I. Gur, V. Zhuang, B. Dai, A. Kumar, R. Agarwal, S. Thiagarajan, C. Boutilier, and A. Faust.  Inference-aware fine-tuning for best-of-n sampling in large language models.  In _The Thirteenth International Conference on Learning Representations_ , 2025.  URL <https://openreview.net/forum?id=77gQUdQhE7>. \n",
    "  * Comanici et al. (2025) G. Comanici, E. Bieber, M. Schaekermann, I. Pasupat, N. Sachdeva, I. Dhillon, M. Blistein, O. Ram, D. Zhang, E. Rosen, et al.  Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities.  _ArXiv preprint_ , abs/2507.06261, 2025.  URL <https://arxiv.org/abs/2507.06261>. \n",
    "  * de Chezelles et al. (2025) T. L. S. de Chezelles, M. Gasse, A. Lacoste, M. Caccia, A. Drouin, L. Boisvert, M. Thakkar, T. Marty, R. Assouel, S. O. Shayegan, L. K. Jang, X. H. Lù, O. Yoran, D. Kong, F. F. Xu, S. Reddy, G. Neubig, Q. Cappart, R. Salakhutdinov, and N. Chapados.  The browsergym ecosystem for web agent research.  _Transactions on Machine Learning Research_ , 2025.  ISSN 2835-8856.  URL <https://openreview.net/forum?id=5298fKGmv3>.  Expert Certification. \n",
    "  * Deng et al. (2023) X. Deng, Y. Gu, B. Zheng, S. Chen, S. Stevens, B. Wang, H. Sun, and Y. Su.  Mind2web: Towards a generalist agent for the web.  In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, _Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023_ , 2023.  URL <http://papers.nips.cc/paper_files/paper/2023/hash/5950bf290a1570ea401bf98882128160-Abstract-Datasets_and_Benchmarks.html>. \n",
    "  * Fang et al. (2025) R. Fang, Y. Liang, X. Wang, J. Wu, S. Qiao, P. Xie, F. Huang, H. Chen, and N. Zhang.  Memp: Exploring agent procedural memory.  _ArXiv preprint_ , abs/2508.06433, 2025.  URL <https://arxiv.org/abs/2508.06433>. \n",
    "  * Fountas et al. (2025) Z. Fountas, M. Benfeghoul, A. Oomerjee, F. Christopoulou, G. Lampouras, H. B. Ammar, and J. Wang.  Human-inspired episodic memory for infinite context LLMs.  In _The Thirteenth International Conference on Learning Representations_ , 2025.  URL <https://openreview.net/forum?id=BI2int5SAC>. \n",
    "  * Gao et al. (2025) H.-a. Gao, J. Geng, W. Hua, M. Hu, X. Juan, H. Liu, S. Liu, J. Qiu, X. Qi, Y. Wu, et al.  A survey of self-evolving agents: On path to artificial super intelligence.  _ArXiv preprint_ , abs/2507.21046, 2025.  URL <https://arxiv.org/abs/2507.21046>. \n",
    "  * Gu et al. (2024) J. Gu, X. Jiang, Z. Shi, H. Tan, X. Zhai, C. Xu, W. Li, Y. Shen, S. Ma, H. Liu, et al.  A survey on llm-as-a-judge.  _ArXiv preprint_ , abs/2411.15594, 2024.  URL <https://arxiv.org/abs/2411.15594>. \n",
    "  * Gur et al. (2024) I. Gur, H. Furuta, A. V. Huang, M. Safdari, Y. Matsuo, D. Eck, and A. Faust.  A real-world webagent with planning, long context understanding, and program synthesis.  In _The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024_. OpenReview.net, 2024.  URL <https://openreview.net/forum?id=9JQtrumvg8>. \n",
    "  * Hu et al. (2025a) M. Hu, T. Chen, Q. Chen, Y. Mu, W. Shao, and P. Luo.  HiAgent: Hierarchical working memory management for solving long-horizon agent tasks with large language model.  In W. Che, J. Nabende, E. Shutova, and M. T. Pilehvar, editors, _Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_ , pages 32779–32798, Vienna, Austria, 2025a. Association for Computational Linguistics.  ISBN 979-8-89176-251-0.  [10.18653/v1/2025.acl-long.1575](https:/doi.org/10.18653/v1/2025.acl-long.1575).  URL <https://aclanthology.org/2025.acl-long.1575/>. \n",
    "  * Hu et al. (2025b) Y. Hu, Y. Wang, and J. McAuley.  Evaluating memory in LLM agents via incremental multi-turn interactions.  In _ICML 2025 Workshop on Long-Context Foundation Models_ , 2025b.  URL <https://openreview.net/forum?id=ZgQ0t3zYTQ>. \n",
    "  * Jimenez et al. (2024) C. E. Jimenez, J. Yang, A. Wettig, S. Yao, K. Pei, O. Press, and K. R. Narasimhan.  Swe-bench: Can language models resolve real-world github issues?  In _The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024_. OpenReview.net, 2024.  URL <https://openreview.net/forum?id=VTF8yNQM66>. \n",
    "  * Jin et al. (2025) C. Jin, H. Peng, Q. Zhang, Y. Tang, D. N. Metaxas, and T. Che.  Two heads are better than one: Test-time scaling of multi-agent collaborative reasoning.  _ArXiv preprint_ , abs/2504.09772, 2025.  URL <https://arxiv.org/abs/2504.09772>. \n",
    "  * Kagaya et al. (2024) T. Kagaya, T. J. Yuan, Y. Lou, J. Karlekar, S. Pranata, A. Kinose, K. Oguri, F. Wick, and Y. You.  Rap: Retrieval-augmented planning with contextual memory for multimodal llm agents.  _ArXiv preprint_ , abs/2402.03610, 2024.  URL <https://arxiv.org/abs/2402.03610>. \n",
    "  * Kong et al. (2025) Y. Kong, D. Shi, G. Yang, C. Huang, X. Li, S. Jin, et al.  Mapagent: Trajectory-constructed memory-augmented planning for mobile task automation.  _ArXiv preprint_ , abs/2507.21953, 2025.  URL <https://arxiv.org/abs/2507.21953>. \n",
    "  * Lee et al. (2025) J. Lee, F. Chen, S. Dua, D. Cer, M. Shanbhogue, I. Naim, G. H. Ábrego, Z. Li, K. Chen, H. S. Vera, et al.  Gemini embedding: Generalizable embeddings from gemini.  _ArXiv preprint_ , abs/2503.07891, 2025.  URL <https://arxiv.org/abs/2503.07891>. \n",
    "  * Li et al. (2025a) D. Li, S. Cao, C. Cao, X. Li, S. Tan, K. Keutzer, J. Xing, J. E. Gonzalez, and I. Stoica.  S*: Test time scaling for code generation.  _ArXiv preprint_ , abs/2502.14382, 2025a.  URL <https://arxiv.org/abs/2502.14382>. \n",
    "  * Li et al. (2025b) Z. Li, S. Song, H. Wang, S. Niu, D. Chen, J. Yang, C. Xi, H. Lai, J. Zhao, Y. Wang, et al.  Memos: An operating system for memory-augmented generation (mag) in large language models.  _ArXiv preprint_ , abs/2505.22101, 2025b.  URL <https://arxiv.org/abs/2505.22101>. \n",
    "  * Liang et al. (2024) X. Liang, Y. He, Y. Xia, X. Song, J. Wang, M. Tao, L. Sun, X. Yuan, J. Su, K. Li, et al.  Self-evolving agents with reflective and memory-augmented abilities.  _ArXiv preprint_ , abs/2409.00872, 2024.  URL <https://arxiv.org/abs/2409.00872>. \n",
    "  * Liu et al. (2025a) B. Liu, X. Li, J. Zhang, J. Wang, T. He, S. Hong, H. Liu, S. Zhang, K. Song, K. Zhu, et al.  Advances and challenges in foundation agents: From brain-inspired intelligence to evolutionary, collaborative, and safe systems.  _ArXiv preprint_ , abs/2504.01990, 2025a.  URL <https://arxiv.org/abs/2504.01990>. \n",
    "  * Liu et al. (2025b) Y. Liu, C. Si, K. R. Narasimhan, and S. Yao.  Contextual experience replay for self-improvement of language agents.  In W. Che, J. Nabende, E. Shutova, and M. T. Pilehvar, editors, _Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_ , pages 14179–14198, Vienna, Austria, 2025b. Association for Computational Linguistics.  ISBN 979-8-89176-251-0.  [10.18653/v1/2025.acl-long.694](https:/doi.org/10.18653/v1/2025.acl-long.694).  URL <https://aclanthology.org/2025.acl-long.694/>. \n",
    "  * Lumer et al. (2025) E. Lumer, A. Gulati, V. K. Subbiah, P. H. Basavaraju, and J. A. Burke.  Memtool: Optimizing short-term memory management for dynamic tool calling in llm agent multi-turn conversations.  _ArXiv preprint_ , abs/2507.21428, 2025.  URL <https://arxiv.org/abs/2507.21428>. \n",
    "  * Madaan et al. (2023) A. Madaan, N. Tandon, P. Gupta, S. Hallinan, L. Gao, S. Wiegreffe, U. Alon, N. Dziri, S. Prabhumoye, Y. Yang, S. Gupta, B. P. Majumder, K. Hermann, S. Welleck, A. Yazdanbakhsh, and P. Clark.  Self-refine: Iterative refinement with self-feedback.  In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, _Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023_ , 2023.  URL <http://papers.nips.cc/paper_files/paper/2023/hash/91edff07232fb1b55a505a9e9f6c0ff3-Abstract-Conference.html>. \n",
    "  * Maharana et al. (2024) A. Maharana, D.-H. Lee, S. Tulyakov, M. Bansal, F. Barbieri, and Y. Fang.  Evaluating very long-term conversational memory of LLM agents.  In L.-W. Ku, A. Martins, and V. Srikumar, editors, _Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_ , pages 13851–13870, Bangkok, Thailand, 2024. Association for Computational Linguistics.  [10.18653/v1/2024.acl-long.747](https:/doi.org/10.18653/v1/2024.acl-long.747).  URL <https://aclanthology.org/2024.acl-long.747/>. \n",
    "  * Miyai et al. (2025) A. Miyai, Z. Zhao, K. Egashira, A. Sato, T. Sunada, S. Onohara, H. Yamanishi, M. Toyooka, K. Nishina, R. Maeda, et al.  Webchorearena: Evaluating web browsing agents on realistic tedious web tasks.  _ArXiv preprint_ , abs/2506.01952, 2025.  URL <https://arxiv.org/abs/2506.01952>. \n",
    "  * Muennighoff et al. (2025) N. Muennighoff, Z. Yang, W. Shi, X. L. Li, L. Fei-Fei, H. Hajishirzi, L. Zettlemoyer, P. Liang, E. Candès, and T. Hashimoto.  s1: Simple test-time scaling.  _ArXiv preprint_ , abs/2501.19393, 2025.  URL <https://arxiv.org/abs/2501.19393>. \n",
    "  * Packer et al. (2023) C. Packer, V. Fang, S. Patil, K. Lin, S. Wooders, and J. Gonzalez.  Memgpt: Towards llms as operating systems.  2023\\. \n",
    "  * Pan et al. (2024) J. Pan, Y. Zhang, N. Tomlin, Y. Zhou, S. Levine, and A. Suhr.  Autonomous evaluation and refinement of digital agents.  In _First Conference on Language Modeling_ , 2024.  URL <https://openreview.net/forum?id=NPAQ6FKSmK>. \n",
    "  * Qian et al. (2024) C. Qian, S. Liang, Y. Qin, Y. Ye, X. Cong, Y. Lin, Y. Wu, Z. Liu, and M. Sun.  Investigate-consolidate-exploit: A general strategy for inter-task agent self-evolution.  _ArXiv preprint_ , abs/2401.13996, 2024.  URL <https://arxiv.org/abs/2401.13996>. \n",
    "  * Setlur et al. (2025) A. Setlur, N. Rajaraman, S. Levine, and A. Kumar.  Scaling test-time compute without verification or RL is suboptimal.  In _Forty-second International Conference on Machine Learning_ , 2025.  URL <https://openreview.net/forum?id=beeNgQEfe2>. \n",
    "  * Shao et al. (2025) R. Shao, R. Qiao, V. Kishore, N. Muennighoff, X. V. Lin, D. Rus, B. K. H. Low, S. Min, W. tau Yih, P. W. Koh, and L. Zettlemoyer.  ReasonIR: Training retrievers for reasoning tasks.  In _Second Conference on Language Modeling_ , 2025.  URL <https://openreview.net/forum?id=kkBCNLMbGj>. \n",
    "  * Shen et al. (2025) J. Shen, H. Bai, L. Zhang, Y. Zhou, A. Setlur, S. Tong, D. Caples, N. Jiang, T. Zhang, A. Talwalkar, et al.  Thinking vs. doing: Agents that reason by scaling test-time interaction.  _ArXiv preprint_ , abs/2506.07976, 2025.  URL <https://arxiv.org/abs/2506.07976>. \n",
    "  * Snell et al. (2025) C. V. Snell, J. Lee, K. Xu, and A. Kumar.  Scaling LLM test-time compute optimally can be more effective than scaling parameters for reasoning.  In _The Thirteenth International Conference on Learning Representations_ , 2025.  URL <https://openreview.net/forum?id=4FWAwZtd2n>. \n",
    "  * SU et al. (2025) H. SU, R. Sun, J. Yoon, P. Yin, T. Yu, and S. O. Arik.  Learn-by-interact: A data-centric framework for self-adaptive agents in realistic environments.  In _The Thirteenth International Conference on Learning Representations_ , 2025.  URL <https://openreview.net/forum?id=3UKOzGWCVY>. \n",
    "  * Sun et al. (2025) Z. Sun, Z. Liu, Y. Zang, Y. Cao, X. Dong, T. Wu, D. Lin, and J. Wang.  Seagent: Self-evolving computer use agent with autonomous learning from experience.  _ArXiv preprint_ , abs/2508.04700, 2025.  URL <https://arxiv.org/abs/2508.04700>. \n",
    "  * Tan et al. (2025) Z. Tan, J. Yan, I.-H. Hsu, R. Han, Z. Wang, L. Le, Y. Song, Y. Chen, H. Palangi, G. Lee, A. R. Iyer, T. Chen, H. Liu, C.-Y. Lee, and T. Pfister.  In prospect and retrospect: Reflective memory management for long-term personalized dialogue agents.  In W. Che, J. Nabende, E. Shutova, and M. T. Pilehvar, editors, _Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_ , pages 8416–8439, Vienna, Austria, 2025. Association for Computational Linguistics.  ISBN 979-8-89176-251-0.  [10.18653/v1/2025.acl-long.413](https:/doi.org/10.18653/v1/2025.acl-long.413).  URL <https://aclanthology.org/2025.acl-long.413/>. \n",
    "  * Tang et al. (2025a) X. Tang, T. Hu, M. Ye, Y. Shao, X. Yin, S. Ouyang, W. Zhou, P. Lu, Z. Zhang, Y. Zhao, A. Cohan, and M. Gerstein.  Chemagent: Self-updating memories in large language models improves chemical reasoning.  In _The Thirteenth International Conference on Learning Representations_ , 2025a.  URL <https://openreview.net/forum?id=kuhIqeVg0e>. \n",
    "  * Tang et al. (2025b) X. Tang, T. Qin, T. Peng, Z. Zhou, D. Shao, T. Du, X. Wei, P. Xia, F. Wu, H. Zhu, et al.  Agent kb: Leveraging cross-domain experience for agentic problem solving.  _ArXiv preprint_ , abs/2507.06229, 2025b.  URL <https://arxiv.org/abs/2507.06229>. \n",
    "  * Wang et al. (2025a) H. Wang, Q. Xu, C. Liu, J. Wu, F. Lin, and W. Chen.  Emergent hierarchical reasoning in llms through reinforcement learning.  _ArXiv preprint_ , abs/2509.03646, 2025a.  URL <https://arxiv.org/abs/2509.03646>. \n",
    "  * Wang et al. (2024) L. Wang, C. Ma, X. Feng, Z. Zhang, H. Yang, J. Zhang, Z. Chen, J. Tang, X. Chen, Y. Lin, et al.  A survey on large language model based autonomous agents.  _Frontiers of Computer Science_ , 18(6):186345, 2024. \n",
    "  * Wang et al. (2025b) Y. Wang, D. Krotov, Y. Hu, Y. Gao, W. Zhou, J. McAuley, D. Gutfreund, R. Feris, and Z. He.  M+: Extending memoryLLM with scalable long-term memory.  In _Forty-second International Conference on Machine Learning_ , 2025b.  URL <https://openreview.net/forum?id=OcqbkROe8J>. \n",
    "  * Wang et al. (2025c) Z. Z. Wang, A. Gandhi, G. Neubig, and D. Fried.  Inducing programmatic skills for agentic tasks.  _ArXiv preprint_ , abs/2504.06821, 2025c.  URL <https://arxiv.org/abs/2504.06821>. \n",
    "  * Wang et al. (2025d) Z. Z. Wang, J. Mao, D. Fried, and G. Neubig.  Agent workflow memory.  In _Forty-second International Conference on Machine Learning_ , 2025d.  URL <https://openreview.net/forum?id=NTAhi2JEEE>. \n",
    "  * Wu et al. (2024a) C. Wu, Z. R. Tam, C. Lin, Y. Chen, and H. Lee.  Streambench: Towards benchmarking continuous improvement of language agents.  In A. Globersons, L. Mackey, D. Belgrave, A. Fan, U. Paquet, J. M. Tomczak, and C. Zhang, editors, _Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024_ , 2024a.  URL <http://papers.nips.cc/paper_files/paper/2024/hash/c189915371c4474fe9789be3728113fc-Abstract-Datasets_and_Benchmarks_Track.html>. \n",
    "  * Wu et al. (2025) D. Wu, H. Wang, W. Yu, Y. Zhang, K.-W. Chang, and D. Yu.  Longmemeval: Benchmarking chat assistants on long-term interactive memory.  In _The Thirteenth International Conference on Learning Representations_ , 2025.  URL <https://openreview.net/forum?id=pZiyCaVuti>. \n",
    "  * Wu et al. (2024b) Y. Wu, Z. Sun, S. Li, S. Welleck, and Y. Yang.  Inference scaling laws: An empirical analysis of compute-optimal inference for problem-solving with language models.  _ArXiv preprint_ , abs/2408.00724, 2024b.  URL <https://arxiv.org/abs/2408.00724>. \n",
    "  * Xie et al. (2024) T. Xie, D. Zhang, J. Chen, X. Li, S. Zhao, R. Cao, T. J. Hua, Z. Cheng, D. Shin, F. Lei, Y. Liu, Y. Xu, S. Zhou, S. Savarese, C. Xiong, V. Zhong, and T. Yu.  Osworld: Benchmarking multimodal agents for open-ended tasks in real computer environments.  In A. Globersons, L. Mackey, D. Belgrave, A. Fan, U. Paquet, J. M. Tomczak, and C. Zhang, editors, _Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024_ , 2024.  URL <http://papers.nips.cc/paper_files/paper/2024/hash/5d413e48f84dc61244b6be550f1cd8f5-Abstract-Datasets_and_Benchmarks_Track.html>. \n",
    "  * Xu et al. (2025) W. Xu, Z. Liang, K. Mei, H. Gao, J. Tan, and Y. Zhang.  A-mem: Agentic memory for llm agents.  _ArXiv preprint_ , abs/2502.12110, 2025.  URL <https://arxiv.org/abs/2502.12110>. \n",
    "  * Yang et al. (2024) J. Yang, C. E. Jimenez, A. Wettig, K. Lieret, S. Yao, K. Narasimhan, and O. Press.  Swe-agent: Agent-computer interfaces enable automated software engineering.  In A. Globersons, L. Mackey, D. Belgrave, A. Fan, U. Paquet, J. M. Tomczak, and C. Zhang, editors, _Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024_ , 2024.  URL <http://papers.nips.cc/paper_files/paper/2024/hash/5a7c947568c1b1328ccc5230172e1e7c-Abstract-Conference.html>. \n",
    "  * Yao et al. (2023) S. Yao, J. Zhao, D. Yu, N. Du, I. Shafran, K. R. Narasimhan, and Y. Cao.  React: Synergizing reasoning and acting in language models.  In _The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023_. OpenReview.net, 2023.  URL <https://openreview.net/pdf?id=WE_vluYUL-X>. \n",
    "  * Yin et al. (2025) S. Yin, J. Guo, K. Shuang, X. Liu, and R. Ou.  Learning wisdom from errors: Promoting llm’s continual relation learning through exploiting error cases.  _ArXiv preprint_ , abs/2508.12031, 2025.  URL <https://arxiv.org/abs/2508.12031>. \n",
    "  * Yu et al. (2025a) H. Yu, T. Chen, J. Feng, J. Chen, W. Dai, Q. Yu, Y.-Q. Zhang, W.-Y. Ma, J. Liu, M. Wang, et al.  Memagent: Reshaping long-context llm with multi-conv rl-based memory agent.  _ArXiv preprint_ , abs/2507.02259, 2025a.  URL <https://arxiv.org/abs/2507.02259>. \n",
    "  * Yu et al. (2025b) X. Yu, B. Peng, V. Vajipey, H. Cheng, M. Galley, J. Gao, and Z. Yu.  ExACT: Teaching AI agents to explore with reflective-MCTS and exploratory learning.  In _The Thirteenth International Conference on Learning Representations_ , 2025b.  URL <https://openreview.net/forum?id=GBIUbwW9D8>. \n",
    "  * Yu et al. (2025c) Z. Yu, Y. Wu, Y. Zhao, A. Cohan, and X.-P. Zhang.  Z1: Efficient test-time scaling with code.  _ArXiv preprint_ , abs/2504.00810, 2025c.  URL <https://arxiv.org/abs/2504.00810>. \n",
    "  * Yue et al. (2025) Y. Yue, Z. Chen, R. Lu, A. Zhao, Z. Wang, S. Song, and G. Huang.  Does reinforcement learning really incentivize reasoning capacity in llms beyond the base model?  _ArXiv preprint_ , abs/2504.13837, 2025.  URL <https://arxiv.org/abs/2504.13837>. \n",
    "  * Zhang et al. (2024a) T. Zhang, A. Madaan, L. Gao, S. Zheng, S. Mishra, Y. Yang, N. Tandon, and U. Alon.  In-context principle learning from mistakes.  In _Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024_. OpenReview.net, 2024a.  URL <https://openreview.net/forum?id=PAPY0cAB3C>. \n",
    "  * Zhang et al. (2025) X. F. Zhang, N. Beauchamp, and L. Wang.  Prime: Large language model personalization with cognitive memory and thought processes.  _ArXiv preprint_ , abs/2507.04607, 2025.  URL <https://arxiv.org/abs/2507.04607>. \n",
    "  * Zhang et al. (2024b) Z. Zhang, Q. Dai, X. Bo, C. Ma, R. Li, X. Chen, J. Zhu, Z. Dong, and J.-R. Wen.  A survey on the memory mechanism of large language model based agents.  _ACM Transactions on Information Systems_ , 2024b. \n",
    "  * Zhao et al. (2024) A. Zhao, D. Huang, Q. Xu, M. Lin, Y. Liu, and G. Huang.  Expel: LLM agents are experiential learners.  In M. J. Wooldridge, J. G. Dy, and S. Natarajan, editors, _Thirty-Eighth AAAI Conference on Artificial Intelligence, AAAI 2024, Thirty-Sixth Conference on Innovative Applications of Artificial Intelligence, IAAI 2024, Fourteenth Symposium on Educational Advances in Artificial Intelligence, EAAI 2014, February 20-27, 2024, Vancouver, Canada_ , pages 19632–19642. AAAI Press, 2024.  [10.1609/AAAI.V38I17.29936](https:/doi.org/10.1609/AAAI.V38I17.29936).  URL <https://doi.org/10.1609/aaai.v38i17.29936>. \n",
    "  * Zheng et al. (2024) L. Zheng, R. Wang, X. Wang, and B. An.  Synapse: Trajectory-as-exemplar prompting with memory for computer control.  In _The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024_. OpenReview.net, 2024.  URL <https://openreview.net/forum?id=Pc8AU1aF5e>. \n",
    "  * Zhong et al. (2024) W. Zhong, L. Guo, Q. Gao, H. Ye, and Y. Wang.  Memorybank: Enhancing large language models with long-term memory.  In M. J. Wooldridge, J. G. Dy, and S. Natarajan, editors, _Thirty-Eighth AAAI Conference on Artificial Intelligence, AAAI 2024, Thirty-Sixth Conference on Innovative Applications of Artificial Intelligence, IAAI 2024, Fourteenth Symposium on Educational Advances in Artificial Intelligence, EAAI 2014, February 20-27, 2024, Vancouver, Canada_ , pages 19724–19731. AAAI Press, 2024.  [10.1609/AAAI.V38I17.29946](https:/doi.org/10.1609/AAAI.V38I17.29946).  URL <https://doi.org/10.1609/aaai.v38i17.29946>. \n",
    "  * Zhou et al. (2024) S. Zhou, F. F. Xu, H. Zhu, X. Zhou, R. Lo, A. Sridhar, X. Cheng, T. Ou, Y. Bisk, D. Fried, U. Alon, and G. Neubig.  Webarena: A realistic web environment for building autonomous agents.  In _The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024_. OpenReview.net, 2024.  URL <https://openreview.net/forum?id=oKn9c6ytLx>. \n",
    "  * Zhou et al. (2025) Z. Zhou, A. Qu, Z. Wu, S. Kim, A. Prakash, D. Rus, J. Zhao, B. K. H. Low, and P. P. Liang.  Mem1: Learning to synergize memory and reasoning for efficient long-horizon agents.  _ArXiv preprint_ , abs/2506.15841, 2025.  URL <https://arxiv.org/abs/2506.15841>. \n",
    "  * Zhu et al. (2025a) K. Zhu, H. Li, S. Wu, T. Xing, D. Ma, X. Tang, M. Liu, J. Yang, J. Liu, Y. E. Jiang, C. Zhang, C. Lin, J. Wang, G. Zhang, and W. Zhou.  Scaling test-time compute for LLM agents.  _ArXiv preprint_ , abs/2506.12928, 2025a.  URL <https://arxiv.org/abs/2506.12928>. \n",
    "  * Zhu et al. (2025b) K. Zhu, H. Li, S. Wu, T. Xing, D. Ma, X. Tang, M. Liu, J. Yang, J. Liu, Y. E. Jiang, et al.  Scaling test-time compute for llm agents.  _ArXiv preprint_ , abs/2506.12928, 2025b.  URL <https://arxiv.org/abs/2506.12928>. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61613034",
   "metadata": {},
   "source": [
    "##  Appendix A Experiment Details\n",
    "\n",
    "This section details the implementation of ReasoningBank with agent systems mentioned in Section [4.1](https://arxiv.org/html/2509.25140v1#S4.SS1 \"4.1 Setup ‣ 4 Experiments ‣ ReasoningBank: Scaling Agent Self-Evolving with Reasoning Memory\") for web browsing tasks including WebArena and Mind2Web. We first present all the prompts used for memory extraction in Appendix [A.1](https://arxiv.org/html/2509.25140v1#A1.SS1 \"A.1 Prompts Used for ReasoningBank ‣ Appendix A Experiment Details ‣ ReasoningBank: Scaling Agent Self-Evolving with Reasoning Memory\"), and then we provide the technical details for memory extraction, retrieval, and consolidation in Appendix [A.2](https://arxiv.org/html/2509.25140v1#A1.SS2 \"A.2 Implementation Details ‣ Appendix A Experiment Details ‣ ReasoningBank: Scaling Agent Self-Evolving with Reasoning Memory\").\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665bd30b",
   "metadata": {},
   "source": [
    "###  A.1 Prompts Used for ReasoningBank\n",
    "\n",
    "![Refer to caption](https://arxiv.org/html/2509.25140v1/x8.png#ai) Figure 8: System instructions for extracting memory items from agent trajectories: the left panel targets successful trajectories (summarizing why they succeed), while the right targets failed trajectories (reflecting on failure and deriving lessons).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e5c0db",
   "metadata": {},
   "source": [
    "##### Memory Extraction.\n",
    "\n",
    "Figure [8](https://arxiv.org/html/2509.25140v1#A1.F8 \"Figure 8 ‣ A.1 Prompts Used for ReasoningBank ‣ Appendix A Experiment Details ‣ ReasoningBank: Scaling Agent Self-Evolving with Reasoning Memory\") illustrates the system instructions we used to guide the extraction of memory items from agent trajectories mentioned in Section [3.2](https://arxiv.org/html/2509.25140v1#S3.SS2 \"3.2 ReasoningBank ‣ 3 Methodology ‣ ReasoningBank: Scaling Agent Self-Evolving with Reasoning Memory\"). We will first obtain correctness signals from LLM-as-a-Judge (Gu et al., [2024](https://arxiv.org/html/2509.25140v1#bib.bib12)) using the same backbone LLMs. When the trajectory corresponds to a successful case (left panel), the instruction emphasizes analyzing why the trajectory led to success and summarizing transferable reasoning strategies. Conversely, when the trajectory represents a failed case (right panel), the instruction requires reflecting on the causes of failure and articulating lessons or preventive strategies. In both settings, the output format is constrained to at most three memory items expressed in a structured Markdown format, ensuring that the resulting insights are concise, non-redundant, and generalizable across tasks rather than tied to specific websites or queries.\n",
    "\n",
    "![Refer to caption](https://arxiv.org/html/2509.25140v1/x9.png#ai) Figure 9: System instructions for obtaining binary signals indicating success or failures of the current trajectory.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529d6e43",
   "metadata": {},
   "source": [
    "##### LLM-as-a-Judge for Correctness Signals.\n",
    "\n",
    "Figure [9](https://arxiv.org/html/2509.25140v1#A1.F9 \"Figure 9 ‣ Memory Extraction. ‣ A.1 Prompts Used for ReasoningBank ‣ Appendix A Experiment Details ‣ ReasoningBank: Scaling Agent Self-Evolving with Reasoning Memory\") displays the instruction used for self-evaluation used to get binary signals for both successes and failures. Given the current user query, trajectory in resolving the query, final state of the website, and model output, the LLM is required to output the state of “Success” or “Failure” of whether the trajectory given successfully resolved the query or not.\n",
    "\n",
    "![Refer to caption](https://arxiv.org/html/2509.25140v1/x10.png#ai) Figure 10: System instructions for memory-aware test-time scaling: the left panel shows parallel scaling (comparing multiple trajectories to extract generalizable insights), while the right panel shows sequential scaling (iteratively re-checking a trajectory to refine the final answer).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68d3510",
   "metadata": {},
   "source": [
    "###  A.2 Implementation Details\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f191f4",
   "metadata": {},
   "source": [
    "##### Memory Extraction.\n",
    "\n",
    "We use an LLM-based extraction pipeline to convert raw trajectories into structured memory items. Specifically, we design a prompt template that asks the model to distill reasoning patterns into three components: title, description, and content as previously mentioned in Appendix [A.1](https://arxiv.org/html/2509.25140v1#A1.SS1 \"A.1 Prompts Used for ReasoningBank ‣ Appendix A Experiment Details ‣ ReasoningBank: Scaling Agent Self-Evolving with Reasoning Memory\"). The backbone LLM of the extractor is set to the same as the agent system with temperature \\(1.0\\). For each trajectory, at most \\(3\\) memory items could be extracted. Crucially, we induce items from _both_ successful and failed trajectories. Successes provide validated strategies, while failures supply counterfactual pitfalls that act as negative signals. To determine success or failure, we adopt an LLM-based binary classifier following (Pan et al., [2024](https://arxiv.org/html/2509.25140v1#bib.bib32); Wang et al., [2025d](https://arxiv.org/html/2509.25140v1#bib.bib47)). The classifier is prompted with the trajectory and the given user query, and asked to output a categorical judgment (Success or Failure) as shown in Figure [9](https://arxiv.org/html/2509.25140v1#A1.F9 \"Figure 9 ‣ Memory Extraction. ‣ A.1 Prompts Used for ReasoningBank ‣ Appendix A Experiment Details ‣ ReasoningBank: Scaling Agent Self-Evolving with Reasoning Memory\"). Similarly, the backbone of the classifier is set to the same as the agent system, with decoding temperature setting to \\(0.0\\) for determinism.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ecc52ab",
   "metadata": {},
   "source": [
    "##### Memory Retrieval and Response Generation.\n",
    "\n",
    "For retrieval, we embed each task query using gemini-embedding-001 (Lee et al., [2025](https://arxiv.org/html/2509.25140v1#bib.bib20)), accessed via Vertex AI.444<https://ai.google.dev/gemini-api/docs/embeddings> Similarity search is conducted over the memory pool using cosine distance. We select memory items of the top-\\(k\\) most similar experiences (default \\(k=1\\); ablation study in §[5.2](https://arxiv.org/html/2509.25140v1#S5.SS2 \"5.2 Incorporating failure trajectories ‣ 5 Analysis ‣ ReasoningBank: Scaling Agent Self-Evolving with Reasoning Memory\")). The retrieved items are concatenated into the agent’s system prompt with a simple formatting template (each item represented by its title and content) and instruction:\n",
    "\n",
    "Below are some memory items that I accumulated from past interaction from the environment that may be helpful to solve the task. You can use it when you feel it’s relevant. In each step, please first explicitly discuss if you want to use each memory item or not, and then take action.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2b3f9d",
   "metadata": {},
   "source": [
    "##### Memory Consolidation.\n",
    "\n",
    "After finishing each new query, the trajectory is processed by the extraction pipeline to produce new memory items, which are appended into the memory pool. We adopt a minimal consolidation strategy: newly generated items are directly added without additional pruning. This choice highlights the contribution of ReasoningBank itself without introducing confounding factors from complex consolidation algorithms. Nevertheless, more advanced consolidation mechanisms (e.g., merging, forgetting) can be incorporated in future work.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b53e875",
   "metadata": {},
   "source": [
    "#####  ReasoningBank Storage\n",
    "\n",
    "We maintain ReasoningBank in a JSON format, and each entry of ReasoningBank consists of a task query, the original trajectory, and the corresponding memory items. All memory items are stored with the schema {title, description, content}. The embedding is pre-computed for each given query and stored in another JSON file for efficient similarity search. We persist the memory pool for each independent run, enabling continual accumulation of experiences throughout test-time learning.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3943613",
   "metadata": {},
   "source": [
    "###  A.3 MaTTS Details\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1492496",
   "metadata": {},
   "source": [
    "##### Prompt Used for MaTTS\n",
    "\n",
    "Figure [10](https://arxiv.org/html/2509.25140v1#A1.F10 \"Figure 10 ‣ LLM-as-a-Judge for Correctness Signals. ‣ A.1 Prompts Used for ReasoningBank ‣ Appendix A Experiment Details ‣ ReasoningBank: Scaling Agent Self-Evolving with Reasoning Memory\") illustrates the system instructions used in our MaTTS framework mentioned in Section [3.3](https://arxiv.org/html/2509.25140v1#S3.SS3 \"3.3 MaTTS: Memory-aware Test-Time Scaling ‣ 3 Methodology ‣ ReasoningBank: Scaling Agent Self-Evolving with Reasoning Memory\"). In the parallel scaling setting (left), multiple trajectories for the same query—both successful and failed—are provided, and the model is instructed to perform self-contrast reasoning. Instead of relying on the LLM to act as an external judge of quality, the model is guided to directly compare and contrast trajectories, identifying patterns that lead to success and mistakes that cause failure. This provides a contrastive signal that grounds the memory extraction process in observable differences between outcomes, yielding more reliable and transferable insights. In the sequential scaling setting (right), the model repeatedly re-examines its own trajectory with check instructions, ensuring consistency and correction over iterations without appealing to external judgment.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6bbf0dd",
   "metadata": {},
   "source": [
    "##### Best-of-N Calculation Details.\n",
    "\n",
    "Given the task query and \\(N\\) trajectories from the agent system, we leverage an LLM and selects the best answer from the \\(N\\) trajectories. The LLM is initiated as the same backbone LLM as the agent system (e.g., if the agent system uses Gemini-2.5-flash, then the model also uses Gemini-2.5-flash). We feed all the \\(N\\) trajectories to the model at once and use a carefully curated prompt shown in Figure [11](https://arxiv.org/html/2509.25140v1#A1.F11 \"Figure 11 ‣ Best-of-N Calculation Details. ‣ A.3 MaTTS Details ‣ Appendix A Experiment Details ‣ ReasoningBank: Scaling Agent Self-Evolving with Reasoning Memory\"), asking the model to select the best answer.\n",
    "\n",
    "![Refer to caption](https://arxiv.org/html/2509.25140v1/x11.png#ai) Figure 11: System instructions for obtaining the best answer from \\(N\\) candidate trajectories for BoN calculation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be92f79",
   "metadata": {},
   "source": [
    "##  Appendix B Details for Experiment Settings\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005be9be",
   "metadata": {},
   "source": [
    "###  B.1 Web Browsing\n",
    "\n",
    "In this section, we detail the experiment settings used for web browsing agents mentioned in Section [4.1](https://arxiv.org/html/2509.25140v1#S4.SS1 \"4.1 Setup ‣ 4 Experiments ‣ ReasoningBank: Scaling Agent Self-Evolving with Reasoning Memory\").\n",
    "\n",
    "Datasets. We test ReasoningBank on three agentic datasets for benchmarking web browsing and coding agents. Specifically, we conduct experiments on WebArena (Zhou et al., [2024](https://arxiv.org/html/2509.25140v1#bib.bib66)) which features general web navigation across diverse domains, spaning shopping, administration, coding (Gitlab), and forums (Reddit). Another benchmark we used is Mind2Web (Deng et al., [2023](https://arxiv.org/html/2509.25140v1#bib.bib8)), which provides playground to test the generalization of agents on versatile operations and environments, including cross-task, cross-website, and cross-domain settings. There are \\(684\\) and \\(1341\\) test instances in total for WebArena and Mind2Web, respectively. For WebArena, the number of instances for different domains are Shopping (187), Admin (182), Gitlab (180), Reddit (106), and Multi (29). For Mind2Web, the number of different settings are Cross-Task (252), Cross-Website (177), and Cross-Domain (912).\n",
    "\n",
    "Baselines. We compare ReasoningBank against several representative memory-augmented approaches: (i) Vanilla, the backbone LLM agent without any memory module, serving as a reference point; (ii) Synapse (Zheng et al., [2024](https://arxiv.org/html/2509.25140v1#bib.bib64)), a representative work that organizes past trajectories as in-context memory; and (iii) AWM (Wang et al., [2025d](https://arxiv.org/html/2509.25140v1#bib.bib47)), which further abstracts common patterns from trajectories into reusable workflows. Together, these baselines span a progression from agents without memory, to those that directly reuse past trajectories, and finally to methods that distill higher-level structures, providing a comprehensive comparison for evaluating ReasoningBank.\n",
    "\n",
    "Implementation Details. We build our agents upon several state-of-the-art LLMs accessed via the Vertex AI API,555<https://cloud.google.com/vertex-ai> including Gemini-2.5-Flash, Gemini-2.5-Pro (Comanici et al., [2025](https://arxiv.org/html/2509.25140v1#bib.bib6)), and Claude-3.7-Sonnet (Anthropic, [2025](https://arxiv.org/html/2509.25140v1#bib.bib1)). These choices allow us to investigate both cross-family (Gemini, Claude) and intra-family (Flash, Pro) variations. BrowserGym (de Chezelles et al., [2025](https://arxiv.org/html/2509.25140v1#bib.bib7)) is used as the execution environment for WebArena, where we set a maximum step limit of \\(30\\) per query. The agent is implemented in ReAct (Yao et al., [2023](https://arxiv.org/html/2509.25140v1#bib.bib54)) style, and iterates until the model predicts the stop action or reaches a task termination condition. We use the decoding temperature of \\(0.7\\) for model generations for both WebArena and Mind2Web.\n",
    "\n",
    "Evaluation Metrics. For WebArena benchmark, we evaluate all methods across two key dimensions: effectiveness and efficiency. For effectiveness, we report the success rate which represents the percentage of user queries successfully resolved by agents. Following the default evaluation protocol of the benchmarks, we employ both LLM-based fuzzy matching and exact string matching to verify whether the essential answer terms appear in the predictions. For efficiency, we measure the average number of steps taken by the agent to complete each query, which reflects the computational and interaction cost incurred during task completion. For Mind2Web dataset, each task in has a predefined fixed number of steps; at each step, the agent needs to predict an action, which is evaluated by: element accuracy: to check if the correct page element is selected, action F1 to check if the action taken on the element is correct. Aggregating element accuracy and action F1 yields step success rate which checks that both element and action selection are correct at the current step. Lastly, after completing every step in the given task, the last metric task-level success rate measures if all intermediate steps are successfully conducted for this task, i.e., all steps for this task score \\(1.0\\) under metric step success rate.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36f2779",
   "metadata": {},
   "source": [
    "###  B.2 Software Engineering\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93babc3c",
   "metadata": {},
   "source": [
    "####  B.2.1 Experiment Setup\n",
    "\n",
    "Datasets. To benchmark agentic coding tasks, we evaluate on SWE-Bench-Verified (Jimenez et al., [2024](https://arxiv.org/html/2509.25140v1#bib.bib16)), a repository-level issue resolution benchmark. The dataset consists of \\(500\\) high-quality test instances that have been manually verified. Each instance requires generating a patch to address the underlying bug described in the input issue. The objective is to modify the relevant portions of the codebase such that all provided test scripts execute successfully.\n",
    "\n",
    "Metrics. We report the issue resolution rate on SWE-Bench-Verified as the primary evaluation metric. The resolution rate measures the percentage of issues successfully fixed across all data points, where an issue is deemed resolved if the submitted patch passes all test scripts. To evaluate the patch application rate, we attempt to apply the generated patches to the repository using the standard patch program, counting only successful applications. Our implementation follows the official evaluation scripts.666<https://www.swebench.com/SWE-bench/api/harness/> For efficiency, we additionally report the average number of steps performed by the agent per instance.\n",
    "\n",
    "Implementation. We implement ReasoningBank for SWE-Bench following the setting of mini-SWE-Agent (Yang et al., [2024](https://arxiv.org/html/2509.25140v1#bib.bib53)), which enforces the Bash-Only environment with no tools and no special scaffold structure. It assumes a simple ReAct agent loop (Yao et al., [2023](https://arxiv.org/html/2509.25140v1#bib.bib54)). Similar to previous experiments, we compare ReasoningBank against (i) No memory and (ii) Synapse. 777We exclude AWM here because the action space in mini-SWE-Agent is open-ended (arbitrary Bash commands), making it difficult to extract the common routines or fixed workflows that AWM requires for cross-task generalization.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91b9846",
   "metadata": {},
   "source": [
    "##  Appendix C Additional Analyses\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8768e23c",
   "metadata": {},
   "source": [
    "###  C.1 Number of Retrieved Experiences\n",
    "\n",
    "We conduct another ablation study on different number of retrieved experiences using Gemini-2.5-flash on WebArena-Shopping subset. As shown in Figure [12](https://arxiv.org/html/2509.25140v1#A3.F12 \"Figure 12 ‣ C.2 Pass@k Analysis ‣ Appendix C Additional Analyses ‣ ReasoningBank: Scaling Agent Self-Evolving with Reasoning Memory\"), we found that incorporating relevant memory significantly boosts performance (from 39.0 without memory to 49.7 with one experience). However, as the number of experiences increases, the success rate gradually declines (46.0 with \\(2\\), 45.5 with \\(3\\), and 44.4 with \\(4\\)). This suggests that while memory provides valuable guidance, excessive experiences may introduce conflicts or noise. Hence, the relevance and quality of memory are more crucial than sheer quantity for effective performance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20e9ebb",
   "metadata": {},
   "source": [
    "###  C.2 Pass@k Analysis\n",
    "\n",
    "![Refer to caption](https://arxiv.org/html/2509.25140v1/x12.png#ai) Figure 12: Ablation results for using various number of experiences.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6679ea",
   "metadata": {},
   "source": [
    "##### Memory-aware scaling improves sample efficiency and sustains stronger performance gains.\n",
    "\n",
    "Pass@\\(k\\) analysis under parallel scaling on WebArena-Shopping subset with Gemini-2.5-flash (Figure [13](https://arxiv.org/html/2509.25140v1#A3.F13 \"Figure 13 ‣ C.3 Case Study ‣ Appendix C Additional Analyses ‣ ReasoningBank: Scaling Agent Self-Evolving with Reasoning Memory\")) reveals two distinct effects. First, MaTTS w/o aggregation (Vanilla TTS) already makes test-time learning behave similarly to RL training: instead of inflating pass@\\(k\\) at large \\(k\\), it improves sample efficiency by guiding exploration. For example, at \\(k=2\\), MaTTS w/o aggregation achieves 50.8 compared to 47.6 from MaTTS w/o memory, extracting more value from each rollout as noted in (Yue et al., [2025](https://arxiv.org/html/2509.25140v1#bib.bib59)). Second, equipping TTS with memory-aware scaling pushes performance further. MaTTS not only preserves efficiency at small \\(k\\) (51.3 at \\(k=2\\)) but also sustains strong growth with scaling, reaching 62.1 at \\(k=5\\), compared to only 52.4 for MaTTS w/o memory. Overall, we see that MaTTS unlocks more potential of agent systems and encourages diversified generation for better pass@\\(k\\) performance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e003596",
   "metadata": {},
   "source": [
    "###  C.3 Case Study\n",
    "\n",
    "![Refer to caption](https://arxiv.org/html/2509.25140v1/x13.png#ai) Figure 13: Pass@\\(k\\) under parallel scaling with ReasoningBank.\n",
    "\n",
    "To better illustrate the benefits of our approach, we present two representative case studies. Figure [14](https://arxiv.org/html/2509.25140v1#A3.F14 \"Figure 14 ‣ C.3 Case Study ‣ Appendix C Additional Analyses ‣ ReasoningBank: Scaling Agent Self-Evolving with Reasoning Memory\") highlights the effectiveness of ReasoningBank in leveraging related previous experiences as memory items. While the baseline agent (without memory) only checks the “Recent Orders” table and mistakenly outputs the most recent purchase date, ReasoningBank recalls from past reasoning hints to explore the full purchase history and correctly identifies the earliest order.\n",
    "\n",
    "Figure [15](https://arxiv.org/html/2509.25140v1#A3.F15 \"Figure 15 ‣ C.3 Case Study ‣ Appendix C Additional Analyses ‣ ReasoningBank: Scaling Agent Self-Evolving with Reasoning Memory\") demonstrates the efficiency gains. In a navigation-heavy shopping task, the baseline requires 29 steps due to repeated inefficient browsing. It stucks and struggles to find the correct place of filter for “Men”. In contrast, ReasoningBank leverages stored reasoning about category filtering, enabling the agent to directly reach the relevant items and complete the task in only 10 steps.\n",
    "\n",
    "![Refer to caption](https://arxiv.org/html/2509.25140v1/x14.png#ai) Figure 14: ReasoningBank enables the agent to recall and apply past reasoning hints, guiding it to the full order history and yielding the correct first purchase date, unlike the baseline that fails with only recent orders. ![Refer to caption](https://arxiv.org/html/2509.25140v1/x15.png#ai) Figure 15: ReasoningBank improves efficiency by leveraging past reasoning hints, reducing the navigation from 29 steps to 10 steps compared to the baseline without memory.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e46d98",
   "metadata": {},
   "source": [
    "##  Appendix D Future Directions\n",
    "\n",
    "In this section, we briefly discuss the potential future directions following ReasoningBank and MaTTS.\n",
    "\n",
    "Compositional Memory. Our current framework distills each experience into multiple memory items, and when a new query arrives, we retrieve similar experiences and reuse all associated items independently. This design highlights the effect of memory content but does not consider how items could be composed into higher-level strategies. Future work could explore composition-aware retrieval and consolidation, enabling the agent to combine complementary items or form reusable macros, thereby yielding richer strategies and stronger generalization in long-horizon tasks.\n",
    "\n",
    "Advanced Memory Architectures. Our current system design is intentionally minimal; a natural next step is to build a layered, product-level memory stack that integrates mature paradigms — e.g., episodic traces (Fountas et al., [2025](https://arxiv.org/html/2509.25140v1#bib.bib10)) for per-task context, short-term “working” memory (Lumer et al., [2025](https://arxiv.org/html/2509.25140v1#bib.bib26)) for within-session state, and long-term (Wang et al., [2025b](https://arxiv.org/html/2509.25140v1#bib.bib45)) consolidated knowledge with decay/refresh policies. The philosophy of ReasoningBank are compatible with the above different memory angularities. Additionally, the current memory retrieval could also move beyond embedding-based similarities to reasoning-intensive controllers (Shao et al., [2025](https://arxiv.org/html/2509.25140v1#bib.bib35)) that decompose queries, plan multi-hop lookups across tiers, and condition selection on uncertainty, recency, and cost. Learning-based routers and consolidation policies could also automate this process. This integration would turn ReasoningBank with MaTTS into a deployable memory service that scales across domains and teams.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01834630",
   "metadata": {},
   "source": [
    "##  Appendix E Limitations\n",
    "\n",
    "While ReasoningBank demonstrates strong empirical performance and introduces a practical paradigm for memory as a scaling dimension, it also comes with several limitations that suggest directions for future research.\n",
    "\n",
    "Focus on memory content. Our study emphasizes how to curate and utilize memory content (e.g., integrating failure trajectories, constructing distilled reasoning cues). For this reason, we did not extensively compare with other memory architectures such as episodic or hierarchical memory. These designs address orthogonal concerns (memory form/structure), while our contribution targets what should be stored and reused. Exploring their combination would be an interesting future direction.\n",
    "\n",
    "Simplicity in memory retrieval and consolidation. We intentionally adopt simple embedding-based retrieval and straightforward consolidation to better isolate the effect of content quality. More sophisticated strategies (e.g., adaptive retrieval, hierarchical consolidation) are compatible with our framework and could further enhance performance, but are not the focus of this work. This choice ensures that the observed gains can be attributed directly to the design of reasoning-oriented memory content.\n",
    "\n",
    "Dependence on LLM-as-a-judge for correctness signals. In our implementation, success and failure signals for trajectories are determined by an LLM-as-a-judge. While this automatic labeling enables scalable evaluation without ground-truth feedback, it may introduce noise when tasks are ambiguous or when the judge model itself errs. While our results suggest the framework remains robust under such noise, future work could incorporate stronger verifiers, human-in-the-loop feedback, or ensemble judgment to enhance the reliability of memory induction.\n",
    "\n",
    "Generated on Mon Sep 29 17:29:07 2025 by [LaTeXML![Mascot Sammy](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==#ai)](http://dlmf.nist.gov/LaTeXML/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2c86b3",
   "metadata": {
    "time_run": "2025-12-06T16:18:13.611213+00:00"
   },
   "outputs": [],
   "source": [
    "from claudette import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c6fc89",
   "metadata": {
    "time_run": "2025-12-06T16:18:32.459318+00:00"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['claude-opus-4-1-20250805',\n",
       " 'claude-sonnet-4-5',\n",
       " 'claude-haiku-4-5',\n",
       " 'claude-opus-4-20250514',\n",
       " 'claude-3-opus-20240229',\n",
       " 'claude-sonnet-4-20250514',\n",
       " 'claude-3-7-sonnet-20250219',\n",
       " 'claude-3-5-sonnet-20241022']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b2450b",
   "metadata": {
    "time_run": "2025-12-06T16:19:08.796714+00:00"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'claude-sonnet-4-5'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = models[1]\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58700a3f",
   "metadata": {
    "solveit_ai": true,
    "use_thinking": true
   },
   "source": [
    "Ok. I would like to implement the ReasoningBank paper to be a solveit tool and would like to think through the best way to approach this. One possibility is as a \"sub-agent\" tool written in claudette. We want the tool to be solveit friendly like dialoghelper and fastcore tools. However, I think this tool would need to be an \"agentic-loop\" wrapper that can be reused with a variety of a set of tools. Look up any information you need like claudette. Consider solveit's tool structure and how you are aware of tools. This is meant to be used as part of a dialog but remember trajectories within dialog tool use.\n",
    "\n",
    "##### 🤖Reply🤖<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠\n",
    "\n",
    "This is a great project! Let me gather some information about claudette's structure first to help us design this well.\n",
    "\n",
    "<details class='tool-usage-details'>\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"id\": \"toolu_01H4F1xoH8CF28ojiydJi2H8\",\n",
    "  \"call\": {\n",
    "    \"function\": \"read_url\",\n",
    "    \"arguments\": {\n",
    "      \"url\": \"https://claudette.answer.ai/\"\n",
    "    }\n",
    "  },\n",
    "  \"result\": \"'[ claudette ](./index.html)\\\\n\\\\n__\\\\n\\\\n  1. [claudette](./index.html)\\\\n\\\\n\\\\n\\\\n  * [ claudette](./index<TRUNCATED>\"\n",
    "}\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "Now let me also look at the toolloop documentation specifically:\n",
    "\n",
    "<details class='tool-usage-details'>\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"id\": \"toolu_01EMZSRhqWVdCvbe8ncgjbfS\",\n",
    "  \"call\": {\n",
    "    \"function\": \"read_url\",\n",
    "    \"arguments\": {\n",
    "      \"url\": \"https://claudette.answer.ai/toolloop.html\"\n",
    "    }\n",
    "  },\n",
    "  \"result\": \"'[ claudette ](./index.html)\\\\n\\\\n__\\\\n\\\\n  1. [Tool loop](./toolloop.html)\\\\n\\\\n\\\\n\\\\n  * [ claudette](./in<TRUNCATED>\"\n",
    "}\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "Excellent! Now I have a good understanding of claudette's structure. Let me think through the design with you.\n",
    "\n",
    "Based on the ReasoningBank paper and claudette's architecture, here's how I'd approach this:\n",
    "\n",
    "**Key ReasoningBank Components to Implement:**\n",
    "\n",
    "1. **Memory Schema**: Each item has `title`, `description`, `content`\n",
    "2. **Memory Store**: Persisted JSON with embeddings for retrieval\n",
    "3. **Memory Retrieval**: Embedding-based similarity search (top-k)\n",
    "4. **Memory Extraction**: LLM prompts to distill from successful AND failed trajectories\n",
    "5. **Memory Consolidation**: Adding new items to the bank\n",
    "\n",
    "**Proposed Architecture for Solveit:**\n",
    "\n",
    "```\n",
    "ReasoningBank (class)\n",
    "├── store: List[MemoryItem]  # The memory items\n",
    "├── embeddings: Dict         # Cached embeddings for retrieval\n",
    "├── retrieve(query, k=1)     # Get relevant memories\n",
    "├── extract(trajectory, success: bool)  # Extract new memories\n",
    "└── consolidate(items)       # Add to bank\n",
    "```\n",
    "\n",
    "**Key Design Questions:**\n",
    "\n",
    "1. **What constitutes a \"trajectory\" in solveit?** In the paper, it's the agent's think/action/observation sequence. For solveit, this could be:\n",
    "   - The full dialog history within a `toolloop` call\n",
    "   - Or specifically the tool calls and their results\n",
    "\n",
    "2. **How should success/failure be determined?** Options:\n",
    "   - LLM-as-a-judge (like the paper)\n",
    "   - User feedback\n",
    "   - Explicit success signal from the task\n",
    "\n",
    "3. **Persistence**: Should the bank persist across dialogs?\n",
    "   - Within a dialog session (ephemeral)\n",
    "   - Across sessions (saved to disk)\n",
    "\n",
    "4. **Integration pattern**: Two approaches:\n",
    "   - **Wrapper approach**: A `ReasoningBankChat` class that wraps claudette's `Chat.toolloop` and automatically handles memory\n",
    "   - **Tool approach**: ReasoningBank as a tool the agent can query/update\n",
    "\n",
    "Which of these design choices resonate with your vision? And what's the primary use case - should this help solveit learn from its own tool use within dialogs, or should it learn from user interactions more broadly?"
   ]
  }
 ],
 "metadata": {
  "solveit_dialog_mode": "learning",
  "solveit_ver": 2
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
